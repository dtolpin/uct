Search problems are often hard to solve in practice because 
 - both the search space is large; 
 - and good heuristics are unknown.
If the search space is relatively small, the problem can be solved
using alpha-beta pruning. If good heuristics are available, only
a small part of the search space is considered before a solution
is found. Otherwise, traditional search algorithms get lost.

Examples of such challenging problems are some computer games, such as 
Computer Go (the search space size is ~10^170), Poker (~10^80), and
many generalization of the shortest path problem, such as 
Canadian Traveller Problem. For comparison, the search space of chess
is `merely' (10^50), and efficient pruning techniques/evaluation
functions exist.

Until recently, the computer was a poor Go player, and many POMDPs
were solved by a human better than the computer (such as driving,
or playing  strategy games). However, there is an approach which
popularity is growing and success is amazing in several areas:
adaptive MCTS, and, in particular UCT (with various improvements
and variations).

The main idea of MCTS is to assess the utility of a move by averaging
outcomes of a number of random rollouts (search simulations) starting
with the move. To increase the chance that such estimate approximates
the true value, `adaptive' MCTS is used, such that seemingly good
moves are sampled more frequently (but bad moves are still checked
occasionally to explore new search directions). 

This formulation of adaptive sampling reminds the Multi-armed Bandits
problem: given a set of arms, each returning a random reward [in a
common setting - from an unknown stationary bounded distribution],  
pull arms in such a way that the total reward is maximized. This
problem is well-known and thoroughly explored: there is a family
algorithms which are said to `solve exploitation/exploration reward'
no matter what the actual reward distributions of the arms are. One
such algorithm is called UCB1 (or simply UCB).

UCT, an algorithm for adaptive MCTS, is based on UCB, namely, it
applies UCB in each node of the rollout, and (in the case of computer
games) mostly simulates moves which look good so far, but occasionally
explores new directions by trying underexplored nodes even if their
average rewards are low. The algorithm has given amazingly good
results in Computer Go agents (MoGo, Fuego, Pachi) as well as in 
some other domains. There is an ongoing effort to adapt UCT to
many different search domains.

However, there is a problem with UCT. While it is true that during
most of the rollout it is desirable to choose `best' moves most of 
the time, the first step is different: discovering that a certain
move/action is bad, is not less important than that it is good:
consider a problem with two actions (and with normally distributed
uncertainty about the utilities) it makes sense to sample the action
with greater uncertainty, whether its average reward is higher or
lower.

Thus, while UCB, an algorithm that maximizes cumulative reward
is suitable for selections starting with the second step of a
simulation layout, an algorithm that optimizes simple reward (that is,
the true utility of the action with the highest average reward) is
better for the first step. An improved version of UCT would look like
the following algorithm:

code of the algorithm here


What are the options for the sampling scheme at the first step?

 - epsilon-greedy sampling (not so bad)
 - a modified version of UCB (worse for cumulative, better for simple
 - reward)
 - sampling based on estimating of VOI for each action, and selecting
   the action with the greatest VOI estimate for sampling.


