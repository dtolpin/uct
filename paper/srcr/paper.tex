%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{algpseudocode}
\usepackage[ruled]{algorithm}
\usepackage{url}
\usepackage{framed}
\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}

\newtheorem{lemma}{Lemma}
\frenchspacing
\pdfinfo{
/Title (MCTS Based on Simple Regret)
/Subject (AAAI Publications)
/Author (AAAI Press)}

\setcounter{secnumdepth}{0}  
\newcommand {\mean} {\ensuremath {\mathop{\mathrm{mean}}}}
\newcommand {\median} {\ensuremath {\mathop{\mathrm{median}}}}
\newcommand {\N} {\ensuremath {\mathcal{N}}}
\newcommand {\IE} {\ensuremath {\mathbb{E}}}
\newcommand {\cov} {\ensuremath {\mathop{\mathrm{cov}}}}
\newcommand {\BEL} {\ensuremath {\mathop{\mathrm{BEL}}}}

\newtheorem{dfn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lmm}{Lemma}
\newtheorem{crl}{Corollary}

%\title{MCTS Based on Simple Regret}
%\author {David Tolpin, Solomon Eyal Shimony \\
%Department of Computer Science, \\
%Ben-Gurion University of the Negev, Beer Sheva, Israel \\
%\{tolpin,shimony\}@cs.bgu.ac.il}

\title{MCTS Based on Simple Regret}
\author {blinded}

\begin{document}

\maketitle

\begin{abstract}
UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS)
in games and Markov decision processes, is based on UCB, a sampling
policy for the Multi-armed Bandit problem (MAB) that asymptotically
minimizes the cumulative regret.  However, search differs from MAB in
that in MCTS it is usually only the final ``arm pull'' (the actual
move selection) that collects a reward, rather than all ``arm pulls''.
Therefore, it makes more sense to minimize the simple regret, as
opposed to the cumulative regret. We begin by introducing policies for
multi-armed bandits with lower finite-time and asymptotic simple
regret than UCB, using it to develop a two-stage scheme (SR+CR) for MCTS
which outperforms UCT empirically.

We then observe that optimizing the sampling process is itself a
meta-reasoning problem, a solution of which can use value of
information (VOI) techniques.  Although the theory of VOI for search
exists, applying it to MCTS is non-trivial, as typical myopic
assumptions fail. Lacking a complete working VOI theory for MCTS, we
nevertheless propose a sampling scheme that is ``aware'' of VOI,
achieving an algorithm that in empirical evaluation outperforms 
both UCT and the other proposed algorithms.
\end{abstract}

\section{Introduction}

Monte-Carlo tree sampling, and especially a version based on the
UCT formula \cite{Kocsis.uct} appears in numerous search applications,
such as \cite{Eyerich.ctp}. Although these methods are shown to be successful empirically,
most authors appear to be using the UCT formula ``because it has been shown
to be successful in the past'', and ``because it does a good job of
trading off exploration and exploitation''. While the latter statement may be
correct for the multi-armed bandit and for the UCB method \cite{Auer.ucb},
we argue that it is inappropriate for search. The problem is not that
UCT does not work; rather, we argue that a simple reconsideration from basic
principles can result in schemes that outperform UCT.

The core issue is that in adversarial search
and search in ``games against nature'' --- optimizing behavior under
uncertainty, the goal is typically to either find a good (or optimal)
strategy, or even to find the best first action of such a policy. Once
such an action is discovered, it is usually not beneficial to further sample
that action, ``exploitation'' is thus meaningless for search
problems. Finding a good first action is closer to the pure
exploration variant, as seen in the selection problem
\cite{Bubeck.pure,TolpinShimony.blinkered}. In the selection problem,
it is much better to minimize the \emph{simple} regret.  However, the
simple and the cumulative regret cannot be minimized simultaneously;
moreover, \cite{Bubeck.pure} shows that in many cases the smaller the
cumulative regret, the greater the simple regret.

We begin with introduction of several sampling schemes with better
bounds for the simple regret on sets. We then extend the results to
sampling in trees by combining the proposed sampling schemes on the
first step of a rollout with UCT during the rest of the
rollout. Finally, we empirically evaluate the performance of the
proposed sampling schemes on sets of Bernoulli arms, in randomly
generated trees, and on the sailing domain.

\section{Background and Related Work}
\label{sec:related-work}

Monte-Carlo tree search was initially suggested as a scheme for
finding approximately optimal policies for Markov Decision Processes (MDP).
An MDP is defined by the set of states $S$, the set of actions $A$, the transition table
$T(s, a, s')$, the reward table $R(s, a, s')$, the initial state $s$
and the goal state $t$: $(S, A, T, R, s, t)$ \cite{Russell.aima}.
Several MCTS schemes explore an MDP by performing \emph{rollouts}---trajectories from the
current state to a state in which a termination condition is satisfied
(either the goal state, or a cutoff state for which the reward is
evaluated approximately). The UCB algorithm (that attempts to minimize
the cumulative regret) \cite{Auer.ucb} had been extended into the tree
search sampling scheme known as UCT \cite{Kocsis.uct}, and shown to outperform
many state of the art search algorithms in both MDP and adversarial search \cite{Eyerich.ctp}. %% more refs

Efficient algorithms for Multi-Armed Bandits based on
distribution-independent bounds, in particular UCB1, or simply UCB, (which is
asymptotically optimal in minimizing the cumulative regret)  were introduced in
\cite{Auer.ucb}. The UCT algorithm, an extension of UCB to
Monte-Carlo Tree Search is described in \cite{Kocsis.uct}. Pure
exploration in Multi-armed bandits is examined in
\cite{Bubeck.pure}. On the one hand, the \cite{Bubeck.pure} prove certain upper
and lower bounds for UCB and uniform sampling, showing that an upper
bound on the simple regret is exponential in the number of samples for
uniform sampling, while only polynomial for UCB. On the other hand,
empirical performance of UCB appears to be better than that of uniform
sampling. 

A completely different direction of controlling sampling can use the
principles of bounded rationality \cite{Horvitz.reasoningabout}
and meta-reasoning ---  \cite{Russell.right} provided a formal
description of rational metareasoning and case studies of applications
in several problem domains. Using the principles of meta-reasoning,
one can ideally find an ``optimal'' sampling scheme, to be used
for selecting what to sample, both at the root node \cite{HayRussell.MCTS} and elsewhere.
However, this task is daunting for the following reasons:
\begin{itemize}
\item Defining the cost of a sample is not easy, and even just
  using time-cost as an approximation, we get an intractable
  meta-reasoning problem.
\item To handle the complexity, one must use simplifying assumptions.
      Applying the standard meta-reasoning myopic assumption, according to 
    which samples would be selected as though at most one sample
   can be taken before an action is chosen, we run into serious problems. Even the basic
  selection problem \cite{TolpinShimony.blinkered} exhibits a
  non-concave utility function and results in premature stopping of the
  standard myopic algorithms. This is due to the fact that the value of information of
  a single measurement (analogous to a sample in MCTS) is frequently
  less than its time-cost, even though this is not true for multiple
  measurements.  When applying the selection problem to MCTS, the
  situation is exacerbated.  The utility of an action is usually
  bounded, and thus in many cases a single sample may be insufficient
  to change the current best action, \emph{regardless} of its
  outcome. As a result, we frequently get a \emph{zero} ``myopic''
  value of information for a single sample.
\item Rational meta-reasoning requires a known distribution model, which may be
  difficult to obtain.
\end{itemize}

As the above ultimate goal is extremely difficult
to achieve, and even harder to analyze, we introduce
simple schemes more amenable to analysis, loosely based on the meta-reasoning
concept of value of information, and compare
them to UCB (on sets) and UCT (in trees).

\section{Sampling Based on Simple Regret}
\label{sec:results}

In the Multi-armed Bandit problem \cite{Vermorel.bandits} we have a set
of $K$ arms. Each arm can be pulled multiple times (sometimes a cost is
associated with each pulling action). When the $i$th arm
is pulled, a random reward $X_i$ from an unknown stationary
distribution is encountered.  The reward is bounded between 0 and 1.
In the cumulative setting, all encountered rewards are collected
by the agent. In the simple (selection) setting, the agent gets to
collect only the reward of the last pull.

\begin{dfn}
The \textbf{simple regret} of a sampling policy for the Multi-armed Bandit
Problem is the expected difference between the best expected reward
$\mu_*$ and the expected reward $\mu_j$ of the empirically best arm
$\overline X_j=\max_i\overline X_i$:
\begin{equation}
\IE r=\sum_{j=1}^K\Delta_j\Pr(\overline X_j=\max_i\overline X_i)
\label{eqn:simple-regret}
\end{equation}
where $\Delta_j=\mu_*-\mu_j$.
\end{dfn}

Strategies that minimize the simple regret are called pure exploration
strategies \cite{Bubeck.pure}. 

\subsection{Sampling on Sets}
\label{sec:sampling-on-sets}

\begin{dfn} Scheme $\mathbf{UCB(\alpha)}$ pulls arm $i$ that maximizes 
upper confidence bound $b_i$ on the reward:
\begin{equation}
b_i=\overline X_i+\sqrt {\frac {\alpha \ln n} {n_i}}
\label{eqn:ucb}
\end{equation}
where $\overline X_i$ is the average reward obtained from arm $i$,
$n_i$ is the number of times arm $i$ was pulled, and $n$ is the total
number of pulls so far. \end{dfn} The best known upper bound on the simple
regret of UCB($\alpha$) is polynomially decreasing in the number of samples
(see \cite{Bubeck.pure}, Theorems~2,3).

An upper bound on the simple regret of uniform sampling is
exponentially decreasing in the number of samples (see
\cite{Bubeck.pure}, Proposition~1). However, empirically
UCB($\alpha$) yields a lower simple regret than uniform
sampling. 

We introduce here two sampling schemes with super-polynomially
decreasing upper bounds on the simple regret. The bounds
suggest that these schemes achieve a lower simple regret
than uniform sampling; indeed, this is confirmed
by experiments. 

We first consider $\varepsilon$-greedy sampling as a straightforward
generalization of uniform sampling:
\begin{dfn} The \textbf{$\mathbf{\varepsilon}$-greedy} sampling scheme
pulls the empirically best arm with probability
$0<\varepsilon<1$ and any other
arm with probability $\frac {1-\varepsilon} {K-1}$. 
\end{dfn}
This sampling scheme exhibits an exponentially decreasing simple regret:
\begin{thm} For any $0<\eta<1$  and $\gamma>1$ there exists $N$ such that for
  any number of samples $n>N$ the simple regret of the  $\varepsilon$-greedy
sampling scheme is  bounded from above as
\begin{equation}
\IE r_{\varepsilon\mbox{-}greedy}\le 2\gamma \sum_{i=1}^K\Delta_i\exp\left(\frac {-2\Delta_j^2n\varepsilon}
  {\left(1+\sqrt{\frac {(K-1)\varepsilon}
        {1-\varepsilon}}\right)^2}\right)
\end{equation}
with probability at least $1-\eta$.
\end{thm}

\begin{proof}[Proof outline:] 

Bound the probability $P_i$ that a non-optimal arm $i$ is selected. Split the interval
 $[\mu_i, \mu_*]$ at $\mu_i+\delta_i$. By Chernoff-Hoeffding bound:
\begin{eqnarray}
P_i&\le&\Pr[\overline X_i>\mu_i+\delta_i]+\Pr[\overline X_*<\mu_*-(\Delta_i-\delta_i)]\nonumber\\
   &\le&\exp\left(-2\delta_i^2n_i\right)+\exp\left(-2(\Delta_i-\delta_i)^2n_*\right)
\end{eqnarray}
Observe that, in probability, $\overline X_i \rightarrow \mu_i$ as $n\rightarrow\infty$, 
therefore $n_*\rightarrow n\varepsilon$, $n_i\rightarrow\frac
{n(1-\varepsilon)} {K-1}$ as $n\rightarrow \infty$. Conclude that for
any $0<\eta<1$, $\gamma>1$ there exists $N$ such that for any $n>N$ and
all non-optimal arms $i$:
\begin{equation}
P_i \le \gamma \left(\exp\left(\frac {-2\delta_i^2n(1-\varepsilon)}{K-1}\right)
+\exp\left(-2(\Delta_i-\delta_i)^2n\varepsilon\right)\right)
\label{eqn:epsgreedy-probbound}
\end{equation}
Require
\begin{eqnarray}
\exp\left(-\frac {2\delta_i^2n(1-\varepsilon)}{K-1}\right)
&=&\exp\left(-2(\Delta_i-\delta_i)^2n\varepsilon\right)\nonumber\\
\frac {\delta_i} {\Delta_i-\delta_i}&=&\sqrt{\frac {(K-1)\varepsilon} {1-\varepsilon}}
\label{eqn:epsgreedy-constant-varepsilon}
\end{eqnarray}
Substitute (\ref{eqn:epsgreedy-probbound}) together with
(\ref{eqn:epsgreedy-constant-varepsilon}) into
(\ref{eqn:simple-regret}) and obtain
\begin{equation}
\IE r_{\varepsilon\mbox{-}greedy}\le 2\gamma \sum_{i=1}^K\Delta_i\exp\left(\frac {-2\Delta_i^2n\varepsilon}
  {\left(1+\sqrt{\frac {(K-1)\varepsilon}
        {1-\varepsilon}}\right)^2}\right)
\end{equation}
\end{proof}

In particular, as the number of arms $K$ grows, the bound for $\frac 1
2$-greedy sampling ($\varepsilon=\frac 1 2$) becomes considerably tighter than for uniform
random sampling ($\varepsilon=\frac 1 K$):
\begin{crl}
For uniform random sampling, 
\begin{equation}
\IE r_{uniform}\le 2\gamma \sum_{i=1}^K\Delta_i\exp\left(-\frac {\Delta_i^2n} {K}\right)
\end{equation}
For $\frac 1 2$-greedy sampling,
\begin{eqnarray}
\IE r_{\frac 1 2\mbox{-}greedy}&\le& 2\gamma \sum_{i=1}^K\Delta_i\exp\left(\frac {-2\Delta_i^2n}
  {\left(1+\sqrt{K-1}\right)^2}\right)\\
  &\approx& 2\gamma \sum_{i=1}^K\Delta_i\exp\left(\frac
    {-2\Delta_i^2n} {K}\right)\mbox{ for }K\gg 1\nonumber
\end{eqnarray}
\end{crl}

$\varepsilon$-greedy is based solely on sampling the empirically best
arm with a higher probability then the rest of the arms, and ignores
information about empirical means of other arms. On the other hand,
UCB distributes samples in accordance with empirical means, but, in order to
minimize cumulative regret, chooses the empirically best arm too often.
Intuitively, a better scheme for simple regret minimization would
distribute samples in a way similar to UCB, but would sample the best arm
less. This can be achieved by replacing $\log(\cdot)$ in
Equation~\ref{eqn:ucb} with a faster growing sublinear function, for
example, $\sqrt\cdot$.
\begin{dfn} Scheme $\mathbf{UCB_{\sqrt{\cdot}}(\alpha)}$ repeatedly pulls arm $i$ that
maximizes $b_i$:
\begin{equation}
b_i=\overline X_i+\sqrt {\frac {\alpha \sqrt n} {n_i}}
\end{equation}
where, as before, $\overline X_i$ is the average reward obtained from arm $i$,
$n_i$ is the number of times arm $i$ was pulled, and $n$ is the total
number of pulls so far. \end{dfn}
This scheme also exhibits a super-polynomially decreasing simple regret:
\begin{thm}  For any $0<\eta<1$  and $\gamma>1$ there exists $N$ such that for
  any number of samples $n>N$ the simple regret of the  UCB$_{\sqrt{\cdot}}$($\alpha$)
sampling scheme is  bounded from above as
\begin{equation}
\IE r_{ucb\sqrt{\cdot}} \le
2\gamma\sum_{i=1}^K\Delta_i\exp\left(-\frac {\alpha\sqrt{n}} 2\right)
\end{equation}
with probability at least $1-\eta$.
\end{thm}

\begin{proof}[Proof outline:] Bound the probability $P_i$ that a
  non-optimal arm $i$ is chosen. Split the interval $[\mu_i, \mu_*]$
  at $\mu_i+\frac {\Delta_i} 2$. By Chernoff-Hoeffding bound:
\begin{eqnarray}
P_i&\le&Pr\left[\overline X_i>\mu_i+\frac {\Delta_i} 2\right]+\Pr\left[\overline X_*<\mu_*-\frac {\Delta_i}
  2\right]\nonumber \\
   &\le&\exp\left(-\frac {\Delta_i^2n_i} 2\right)+\exp\left(-\frac {\Delta_i^2n_*} 2\right)
\end{eqnarray}
Observe that, in probability, $n_i\to \frac {\alpha \sqrt n}
{\Delta_i^2}$, $n_i\le n_*$ as $n\to\infty$. Conclude that for
any $0<\eta<1$, $\gamma>1$ there exists $N$ such that for any $n>N$ and
all non-optimal arms $i$:
\begin{equation}
P_i \le 2\gamma \exp\left(-\frac {\alpha \sqrt n} 2\right)
\label{eqn:usbsqrt-probbound}
\end{equation}
Substitute (\ref{eqn:usbsqrt-probbound}) into
(\ref{eqn:simple-regret}) and obtain
\begin{equation}
\IE r_{ucb\sqrt{\cdot}} \le 2\gamma\sum_{i=1}^K\Delta_i\exp\left(-\frac {\alpha\sqrt{n}} 2\right)
\end{equation}
\end{proof}

\subsection{Sampling in Trees}
\label{sec:sampling-in-trees}

UCT \cite{Kocsis.uct} is a generalization of UCB for MCTS.  UCT
applies UCB at each step of a rollout.  
At the root node, the goal of sampling in MCTS is usually finding the
first action to perform. Search is re-started, either from scratch or using some
previously collected information, after observing the actual outcome (in MDPs) or
the opponent's move (in adversarial games). As in many domains neither can be
predicted with any degree of certainty, there is usually little utility in performing additional
sampling once one action is shown to be the best choice with high confidence.
As the choice of move is unlikely to change, the value of information of additional samples
is low. Therefore, one should be able to do better than UCT by optimizing {\em simple regret},
rather than {\em cumulative regret},
at the root node.

For nodes deeper in the search tree the situation is not as clear, but here the task appears different.
In order to support an optimal move choice at the root, a precise {\em value} of the search
tree nodes is beneficial. For these internal nodes optimizing simple regret is not the answer, and 
cumulative regret optimization seems more reasonable. Lacking a complete meta-reasoning for sampling,
which would indicate the optimal way to sample both root nodes and internal nodes,
our suggested improvement to UCT thus combines different sampling schemes on the first step and
during the rest of each rollout:
\begin{dfn}
The \textbf{SR+CR MCTS sampling scheme} selects an action at the
current root node according to a scheme suitable for minimizing 
the simple regret (\textbf{SR}), such as $\frac 1 2$-greedy or UCB$_{\sqrt{\cdot}}$, and
then selects actions according to UCB, minimizing the cumulative regret (\textbf{CR}).
\end{dfn}

Such two-stage sampling scheme outperforms UCT, and, additionally, is
significantly less sensitive to the tuning of the exploration factor
$\alpha$ of UCT, since the conflict between the need for a larger
value of $\alpha$ on the first step (simple regret) and a smaller
value for the rest of the rollout (cumulative regret)
\cite{Bubeck.pure} is resolved. In fact, a sampling scheme that uses
UCB at all steps but a larger value of $\alpha$ for the first step
than for the rest of the steps, outperforms UCT. The pseudocode of the
two-stage rollout is in Algorithm~\ref{alg:two-stage-mcts}.

\begin{algorithm}[h!]
\caption{Two-stage Monte-Carlo tree search sampling}
\label{alg:two-stage-mcts}
\begin{algorithmic}[1]
\Procedure{Rollout}{node, depth=1}
  \If {\Call{IsLeaf}{node, depth}}
    \State \textbf{return} 0
  \Else
    \If {depth=1}
      \State action $\gets$ \Call{FirstAction}{node}
    \Else
      \State action $\gets$ \Call{NextAction}{node}
    \EndIf
    \State next-node $\gets$ \Call{NextState}{node, action}
    \State reward $\gets$ \Call{Reward}{node, action, next-node}
     \State \hspace{4em} + \Call{Rollout}{next-node, depth+1}
    \State \Call{UpdateStats}{node, action, reward}
  \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{VOI-aware Sampling}
\label{sec:voi-sampling}

Further improvement can be achieved by computing or estimating the
value of information (VOI) of the rollouts and choosing rollouts that
maximize the VOI. This is done by maintaining a current best move at the root,
and finding the expected gain from finding another move to be better than the
current best \cite{Russell.aima}.
However, as indicated above, actually computing the VOI is infeasible.
Instead we suggest the following scheme based on the following
features of value of information:
\begin{enumerate}
\item An estimate of the probability that one or more rollouts will make another action
appear better than the current best.
\item An estimate of the gain that may be incurred if such a change occurs.
\end{enumerate}

If the distribution of results generated by the rollouts were known, the
above features could be easily computed. However, this is not the case for
most MCTS applications. Therefore, we estimate bounds on the feature values from
the current set of samples, based on the myopic assumption that the algorithm will only
sample one of the actions, and use these bounds as the feature values, to get:
\begin{eqnarray}
VOI_\alpha&\approx&\frac {\overline X_\beta} {n_\alpha+1}
\exp\left(-2(\overline X_\alpha - \overline X_\beta)^2 n_\alpha\right)\\
VOI_i&\approx&\frac {1-\overline X_\alpha} {n_i+1}
\exp\left(-2(\overline X_\alpha - \overline X_i)^2 n_i\right),\; i\ne\alpha\nonumber\\
\mbox{where }&&\alpha=\arg\max_i \overline X_i,\quad
             \beta=\arg\max_{i,\,i\ne\alpha} \overline X_i\nonumber
\end{eqnarray}
with $VOI_{\alpha}$ being (approximate) value for sampling the current best action,
and $VOI_i$ is the (approximate) value for sampling some other action $i$. 

These equations were derived as follows. The gain from switching from the current best action $\alpha$ to another
action can be bounded:  by  the current expectation of the value the current second-best action
for the case where we sample only $\alpha$, and by 1 (the maximum reward) minus the current expectation
of $\alpha $ when sampling any other action. 
The probability that another action be found
best can be bounded by an exponential function of the difference in expectations when the true value
of the actions becomes known. But the effect of each individual sample on the sample mean
is inversely proportional to the current number of samples, hence the current number of samples (plus one
in order to handle the initial case of no previous samples) in the denominator.
Early experiments with this approach demonstrate a significantly lower simple regret.

\section{Empirical Evaluation}
\label{sec:emp}

The results were empirically verified on Multi-armed Bandit instances,
on search trees, and on the sailing domain, as defined in
\cite{Kocsis.uct}. In most cases, the experiments showed a lower average
simple regret for $\frac 1 2$-greedy an UCB$_{\sqrt{\cdot}}$ than for
UCB on sets, and for the SR+CR scheme than for UCT in trees.

\subsection{Simple regret in multi-armed bandits}
\label{sec:emp-mab}

Figure~\ref{fig:mab-simple-regret} presents a comparison of MCTS sampling
schemes on Multi-armed bandits. Figure~\ref{fig:mab-simple-regret}.a shows the search tree
corresponding to a problem instance. Each arm returns a random reward
drawn from a Bernoulli distribution. The search selects an arm
and compares the expected reward, unknown to the algorithm during the
sampling, to the expected reward of the best arm.

Figure~\ref{fig:mab-simple-regret}.b shows the regret
vs. the number of samples, averaged over $10^4$ experiments for
randomly generated instances of 32 arms. For smaller numbers of
samples, $\frac 1 2$-greedy achieves the best
performance; for larger numbers of samples, UCB$_{\sqrt{\cdot}}$
outperforms $\frac 1 2$-greedy. A combination of $\frac 1
2$-greedy  and UCB$_{\sqrt{\cdot}}$ dominates UCB over the
whole range.

\begin{figure}[h!]
  \begin{minipage}[c]{1.0\linewidth}
    \centering
    \includegraphics[scale=0.9]{onelevel-tree.pdf}\\
    a. search tree
    \vspace{1em}
  \end{minipage}
  \begin{minipage}[c]{1.0\linewidth}
    \centering
    \includegraphics[scale=0.45]{flat-trilevel-k=64-uqb=8.pdf}\\
    b. regret vs. number of samples
  \end{minipage}
  \caption{Simple regret in MAB}
  \label{fig:mab-simple-regret}
\end{figure}

\subsection{Monte Carlo tree search}
\label{sec:emp-mcts}

The second set of experiments was performed on randomly generated
trees crafted in such a way that uniform random sampling selects a
direction at the root randomly. The degree of the root is a parameter of
the tree generator. The degree of all nodes at distance 1 from the
root is 2, and all nodes at distance 2 from the roots are leaves. The
average reward of two children of each node at distance 1 is
0.5. Thus, a uniform sampling scheme results in the same average reward for
all edges at the root, and an adaptive sampling scheme, such as UCT,
has to be used.

Figure~\ref{fig:mcts-regret} shows a sketch of the search tree
(Figure~\ref{fig:mcts-regret}.a) and the dependency of the regret vs. the
number of samples for trees with root degree 16
(Figure~\ref{fig:mcts-regret}.b) and 64 (Figure~\ref{fig:mcts-regret}.c). The
dependencies look differently from Multi-armed bandit instances, but
the algorithms exhibit a similar relative performance: either $\frac 1
2$-greedy+UCT or UCB$_{\sqrt{\cdot}}$+UCT
gives the lowest regret, UCB$_{\sqrt{\cdot}}$+UCT dominates UCT everywhere
except for small numbers of instances. The advantage of both $\frac 1
2$-greedy+UCT and UCB$_{\sqrt{\cdot}}$+UCT grows with the number of arms.

\begin{figure}[h!]
  \begin{minipage}[c]{1.0\linewidth}
    \centering
    \includegraphics[scale=0.7]{twolevel-tree.pdf}\\
    a. search tree
    \vspace{0.5em}
  \end{minipage}
  \begin{minipage}[c]{1.0\linewidth}
    \centering
    \includegraphics[scale=0.45]{tree-identity-k=16-uqb=8.pdf}\\ 
    b. 16 arms
    \vspace{0.5em}
  \end{minipage}
  \begin{minipage}[c]{1.0\linewidth}
    \centering
    \includegraphics[scale=0.45]{tree-identity-k=64-uqb=8.pdf} \\
    c. 64 arms
  \end{minipage}
  \caption{MCTS in random trees}
  \label{fig:mcts-regret}
\end{figure}

\subsection{The sailing domain}
\label{sec:emp-sailing}

Figures~\ref{fig:sailing-cost-vs-nsamples}--\ref{fig:sailing-lake-size}
show results of experiments on the sailing
domain. Figure~\ref{fig:sailing-cost-vs-nsamples} shows the regret
vs. the number of samples, computed for a range of values of
$\alpha$. Figure~\ref{fig:sailing-cost-vs-nsamples}.a shows the median
cost, and Figure~\ref{fig:sailing-cost-vs-nsamples}.b --- the minimum
costs. UCT is always worse than either $\frac 1 2$-greedy+UCT or
UCB$_{\sqrt{\cdot}}$+UCT, and is sensitive to the value of $\alpha$: the median cost is
much higher than the minimum cost for UCT. For both $\frac 1 2$-greedy+UCT
and UCB$_{\sqrt{\cdot}}$+UCT, the difference is significantly less prominent.

\begin{figure}[h!]
  \begin{minipage}[b]{1.0\linewidth}
    \centering
    \includegraphics[scale=0.425]{costs-size=6-group=median.pdf}\\
    a. median cost
    \vspace{0.5em}
  \end{minipage}
  \begin{minipage}[b]{1.0\linewidth}
    \centering
    \includegraphics[scale=0.425]{costs-size=6-group=minimum.pdf}\\
    b. minimum cost
  \end{minipage}
  \caption{The sailing domain, $6\times 6$ lake, cost vs. samples}
  \label{fig:sailing-cost-vs-nsamples}
\end{figure}

Figure~\ref{fig:sailing-cost-vs-factor} shows the regret vs. the
exploration factor for different numbers of samples. UCB$_{\sqrt{\cdot}}$+UCT is always better than
UCT, and $\frac 1 2$-greedy+UCT is better than UCT expect for a small range of
values of the exploration factor $\alpha$. 

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.425]{costs-size=6-nsamples=397.pdf}\\
  a. 397 rollouts\\
  \vspace{0.5em}
  \includegraphics[scale=0.425]{costs-size=6-nsamples=1585.pdf}\\
  c. 1585 rollouts
  \caption{The sailing domain, $6\times 6$ lake, cost vs. factor}
  \label{fig:sailing-cost-vs-factor}
\end{figure}

Figure~\ref{fig:sailing-lake-size} shows the cost vs. the exploration
factor for lakes of different sizes. The relative difference between
the sampling schemes becomes more prominent when the lake size
increases.
\begin{figure}[h!]
   \centering
   \includegraphics[scale=0.425]{costs-size=6-nsamples=397.pdf}\\
   b. $6\times 6$ lake \\
   \vspace{0.5em}
   \includegraphics[scale=0.425]{costs-size=10-nsamples=397.pdf}\\
   c. $10\times 10$ lake
  \caption{The sailing domain, 397 samples, cost vs. factor}
  \label{fig:sailing-lake-size}
\end{figure}

\subsection{VOI-aware MCTS}

Finally, the VOI-aware sampling scheme was empirically compared to
other sampling schemes (UCT, $\frac 1 2$-greedy+UCT,
UCT$_{\sqrt{\cdot}}$+UCT). Again, the experiments were performed on
randomly generated trees with structure shown in
Figure~\ref{fig:mcts-regret}.a. Figure~\ref{fig:mcts-regret-voi} shows
the results for 32 arms. VOI+UCT, the scheme based on a VOI estimate,
outperforms all other sampling schemes in this example. 
\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.45]{tree-identity-k=32-uqb=8+voi.pdf}\\
  \caption{MCTS in random trees, VOI+UCT vs. other schemes.}
  \label{fig:mcts-regret-voi}
\end{figure}

\section{Summary and Future Work}
\label{sec:summary}

UCT-based Monte-Carlo tree search has been shown to be very effective
for finding good actions in both MDPs and adversarial games.
Further improvement of the sampling scheme is thus of interest in
numerous search applications. We argue that although UCT is already very efficient,
one can do better if the sampling scheme is considered from a meta-reasoning
perspective of value of information (VOI).

The MCTS SR+CR scheme presented in the paper differs
from UCT mainly in the first step of the rollout, when the `simple' selection
regret is minimized instead of the cumulative regret. Both the
theoretical analysis and the empirical evaluation provide evidence for
better general performance of the proposed scheme.

Although SR+CR is inspired  by the notion of VOI,
the VOI is used there implicitly in the analysis of the algorithm,
rather than computed or learned explicitly in order to plan the
rollouts. Ideally, using VOI to control sampling ab-initio should do even better,
but the theory for doing that is still not up to speed. Instead we suggest
a  ``VOI-aware'' sampling scheme based on crude probability and value estimates,
which despite its simplicity already shows a marked improvement in minimizing regret.
However, application of the theory of rational metareasoning
to Monte Carlo Tree Search is an open problem \cite{HayRussell.MCTS},
and both a solid theoretical model and empirically efficient VOI
estimates need to be developed.

\section*{Acknowledgments}

Blinded.

%The research is partially supported by Israel
%Science Foundation grant 305/09, by the Lynne and William Frankel
%Center for Computer Sciences, and by the Paul Ivanier Center for
%Robotics Research and Production Management.

\bibliographystyle{aaai}
\bibliography{refs}


\end{document}
