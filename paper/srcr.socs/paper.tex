\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{algpseudocode}
\usepackage[ruled]{algorithm}
\usepackage{url}
\usepackage{framed}
\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}

\newtheorem{lemma}{Lemma}
\frenchspacing
\pdfinfo{
/Title (MCTS Based on Simple Regret)
/Subject (AAAI Publications)
/Author (AAAI Press)}

\setcounter{secnumdepth}{0}  
\newcommand {\mean} {\ensuremath {\mathop{\mathrm{mean}}}}
\newcommand {\median} {\ensuremath {\mathop{\mathrm{median}}}}
\newcommand {\N} {\ensuremath {\mathcal{N}}}
\newcommand {\IE} {\ensuremath {\mathbb{E}}}
\newcommand {\cov} {\ensuremath {\mathop{\mathrm{cov}}}}
\newcommand {\BEL} {\ensuremath {\mathop{\mathrm{BEL}}}}

\newtheorem{dfn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lmm}{Lemma}
\newtheorem{crl}{Corollary}

\title{MCTS Based on Simple Regret}
\author {David Tolpin, Solomon Eyal Shimony \\
Department of Computer Science, Ben-Gurion University of the Negev, Israel \\
\{tolpin,shimony\}@cs.bgu.ac.il}

\title{MCTS Based on Simple Regret}

\begin{document}

\maketitle

\begin{abstract}
UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS),
is based on UCB, a policy for the Multi-armed Bandit problem (MAB) that 
minimizes the cumulative regret.  However, search differs from MAB in
that in MCTS it is usually only the final ``arm pull''
that collects a reward, rather than all ``arm pulls''.
Therefore, it makes more sense to minimize the simple, rather than
cumulative, regret. We  introduce policies for
multi-armed bandits with lower simple
regret than UCB and develop a two-stage scheme (SR+CR) for MCTS
which outperforms UCT empirically. We also propose a sampling
scheme based on value of information (VOI), achieving an algorithm
that empirically outperforms other proposed algorithms.
\end{abstract}

\section{Introduction}

Monte-Carlo tree search, and especially a version based on the UCT
formula \cite{Kocsis.uct} appears in numerous search applications,
such as \cite{GellyWang.mogo,Eyerich.ctp}. Although these methods are
shown to be successful empirically, we argue that a simple
reconsideration from basic principles can result in schemes that
outperform UCT.

In adversarial search and optimization under uncertainty, the goal is
typically to either find a good strategy, or even just to
find the best first action of such a strategy, which is closer to the pure
exploration variant, as seen in the selection problem
\cite{Bubeck.pure,TolpinShimony.blinkered}. In the selection problem,
it is much better to minimize the \emph{simple} regret.  However, the
simple and the cumulative regret cannot be minimized
simultaneously \cite{Bubeck.pure}.

In the {\bf Multi-armed Bandit problem} \cite{Vermorel.bandits} we have a set
of $K$ arms. Each arm can
be pulled multiple times. When the $i$th arm
is pulled, a random reward $X_i$ from an unknown stationary
distribution is encountered.
In the cumulative setting (the focus of much of the research literature on Multi-armed bandits),
all encountered rewards are collected by the agent. 
In this respect, the $\mathbf{UCB(c)}$ scheme that pulls arm $i$ maximizing
upper confidence bound $b_i$ on the reward 
\begin{equation}
b_i=\overline X_i+\sqrt {{c \log (n)} / {n_i}}
\label{eqn:ucb}
\end{equation}
is near-optimal \cite{Auer.ucb}.

The {\bf UCT algorithm,} an extension of UCB
to Monte-Carlo Tree Search \cite{Kocsis.uct} is
shown to outperform many state of the art search algorithms in both
MDP and adversarial games \cite{Eyerich.ctp,GellyWang.mogo}. 
However, the best known upper bound on the simple regret of UCB is
only polynomially decreasing in the number of samples \cite{Bubeck.pure}.

A completely different scheme for control of sampling can use the
principles of {\bf bounded rationality} \cite{Horvitz.reasoningabout}
and {\bf rational metareasoning} \cite{Russell.right}. We introduce in this paper simple
schemes loosely based on the metareasoning concept of value of
information (VOI), and compare them to UCB (on sets) and UCT (in trees).


\section{Sampling Based on Simple Regret}
\label{sec:results}

\subsection{Analysis of Sampling on Sets}
\label{sec:sampling-on-sets}


We examine some sampling schemes with super-polynomially
decreasing upper bounds on the simple regret. The bounds
suggest that these schemes achieve a lower simple regret
than uniform sampling; indeed, this is confirmed
by experiments. First, we consider $\varepsilon$-greedy sampling, which
pulls the arm that currently has the greatst sample mean, with probability
$0<\varepsilon<1$, and any other arm with probability $\frac {1-\varepsilon} {K-1}$. 
This sampling scheme exhibits an exponentially decreasing simple
regret. Moreover, as the number of arms $K$ grows, the bound for $\frac 1
2$-greedy sampling ($\varepsilon=\frac 1 2$) becomes considerably tighter than for uniform
random sampling ($\varepsilon=\frac 1 K$).

Another sampling scheme for simple regret minimization can be obtained
by distributing samples in a way similar to UCB, but sampling the current best arm
less often. This can be achieved by replacing $\log(\cdot)$ in
Equation~\ref{eqn:ucb} with $\sqrt\cdot$. This scheme also exhibits
a super-polynomially decreasing simple regret.

\subsection{Sampling in Trees}
\label{sec:sampling-in-trees}

As mentioned above, UCT \cite{Kocsis.uct} is an extension of UCB for
MCTS, that applies UCB($c$) at each step of a rollout.  At the root
node, the sampling in MCTS is usually aimed at finding the first move
to perform. Search is re-started, either from scratch or using some
previously collected information, after observing the actual outcome
(in MDPs) or the opponent's move (in adversarial games). Therefore,
one should be able to do better than UCT by optimizing {\em simple}
rather than {\em cumulative regret}, at the root node.

However, for thee internal nodes optimizing simple regret is not the
answer, and cumulative regret optimization is not so far off the
mark. Our suggested improvement to UCT is the \textbf{SR+CR MCTS
sampling scheme} that combines different sampling schemes on the first
step and during the rest of each rollout.  We expect such two-stage
sampling schemes to outperform UCT.


\subsection{VOI-aware Sampling}
\label{sec:voi-sampling}

Further improvement can be achieved by computing or estimating the
VOI of the rollouts and choosing rollouts that
maximize the VOI. We estimate the VOI from
the current set of samples using distribution-independent concentration inequalities.
%\begin{eqnarray}
%VOI_\alpha&\approx& \frac {\overline X_\beta} {n_\alpha+1}
%\exp\left(-2(\overline X_\alpha - \overline X_\beta)^2 n_\alpha\right)\\
%VOI_i&\approx& \frac {1-\overline X_\alpha} {n_i+1}
%\exp\left(-2(\overline X_\alpha - \overline X_i)^2 n_i\right),\; i\ne\alpha\nonumber\\
%\mbox{where }&&\alpha=\arg\max_i \overline X_i,\quad
%             \beta=\arg\max_{i,\,i\ne\alpha} \overline X_i\nonumber
%\end{eqnarray}
The ``VOI-aware'' scheme samples the action that has the maximum
estimated VOI.  Although this scheme appears too complicated for
a formal analysis, early experiments demonstrate a significantly lower
simple regret.


\section{Empirical Evaluation}
\label{sec:emp}

The results were empirically verified on Multi-armed Bandit instances,
on search trees, and on the sailing domain
\cite{Kocsis.uct}. In most cases, the experiments showed a lower average
simple regret for $\frac 1 2$-greedy an UCB$_{\sqrt{\cdot}}$ than for
UCB on sets, and for the SR+CR scheme than for UCT in trees.

Figure~\ref{fig:mcts-regret} shows
results for experiments on randomly generated trees. VOI+UCT, the scheme based on a VOI estimate,
outperforms all other sampling schemes in this example; either $\frac 1
2$-greedy+UCT or UCB$_{\sqrt{\cdot}}$+UCT
result in the second lowest regret, UCB$_{\sqrt{\cdot}}$+UCT dominates UCT everywhere
except when the number of samples is small. The advantage of both $\frac 1
2$-greedy+UCT and UCB$_{\sqrt{\cdot}}$+UCT grows with the number of arms.
\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.5]{tree-identity-k=32-uqb=8+voi.pdf}\\
  \caption{MCTS in random trees.}
  \label{fig:mcts-regret} 
\end{figure}

%Figure~\ref{fig:sailing-cost-vs-nsamples}
%shows results of experiments on the sailing
%domain. UCT is always worse than either $\frac 1 2$-greedy+UCT or
%UCB$_{\sqrt{\cdot}}$+UCT.
%
%\begin{figure}[h!]
%  \centering
%  \includegraphics[scale=0.45]{costs-size=6-group=median.pdf}\\
%  \caption{The sailing domain, $6\times 6$ lake, cost vs. samples}
%  \label{fig:sailing-cost-vs-nsamples}
%\end{figure}


\section{Conclusion and Future Work}
\label{sec:summary}

UCT-based Monte-Carlo tree search has been shown to be very effective
for finding good actions in both MDPs and adversarial games.
Both the theoretical analysis and the empirical evaluation provide evidence for
better general performance of the proposed SR+CR scheme.

Using VOI to control sampling ab-initio should provide further
improvement. However, application of the theory of rational
metareasoning to MCTS is an open problem, and both a solid theoretical
model and empirically efficient VOI estimates need to be developed.

Applying VOI methods in complex deployed applications that
already use MCTS should be addressed.  In particular, UCT is extremely
successful in Computer Go
\cite{GellyWang.mogo,Braudis.pachi,Enzenberger.Fuego}, and the
proposed scheme should be evaluated on this domain. This is
non-trivial, since Go programs typically use ``non-pure'' versions of
UCT.


\section*{Acknowledgments}

The research is partially supported by Israel
Science Foundation grant 305/09, by the Lynne and William Frankel
Center for Computer Sciences, and by the Paul Ivanier Center for
Robotics Research and Production Management.

\bibliographystyle{aaai}
\bibliography{refs}


\end{document}
