\documentclass{article}
\usepackage{algpseudocode}
\usepackage[ruled]{algorithm}
\usepackage{url}
\usepackage{framed}
\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}
\usepackage{geometry}

\geometry{margin=1.2in}

\newcommand {\mean} {\ensuremath {\mathop{\mathrm{mean}}}}
\newcommand {\median} {\ensuremath {\mathop{\mathrm{median}}}}
\newcommand {\N} {\ensuremath {\mathcal{N}}}
\newcommand {\IE} {\ensuremath {\mathbb{E}}}
\newcommand {\cov} {\ensuremath {\mathop{\mathrm{cov}}}}
\newcommand {\BEL} {\ensuremath {\mathop{\mathrm{BEL}}}}

\newtheorem{dfn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lmm}{Lemma}

\title{VOI-aware Monte Carlo Sampling in Trees}
\author {David Tolpin, Solomon Eyal Shimony \\
Department of Computer Science, \\
Ben-Gurion University of the Negev, Beer Sheva, Israel \\
\{tolpin,shimony\}@cs.bgu.ac.il}

\begin{document}

\maketitle

\begin{abstract}
Upper bounds for the VOI are provided for pure exploration in the
Multi-armed Bandit Problem. Sampling policies based on the upper
bounds are suggested. Empirical evaluation of the policies and
comparison to the UCB1 and UCT policies is provided
on random problem instances as well as on the Go game.
\end{abstract}


\section{Introduction and Definitions}

Taking a sequence of samples in order to minimize the
regret of a decision based on the samples is abstracted by the
{\em Multi-armed Bandit Problem.} In the Multi-armed Bandit problem
we have a set of $K$ arms. Each arm can be pulled multiple
times. When the $i$th arm is pulled, a random reward $X_i$ from an
unknown stationary distribution is returned.  The reward is bounded
between 0 and 1.

The simple regret of a sampling policy for the Multi-armed Bandit
Problem is the expected difference between the best expected reward
$\mu_*$ and the expected reward $\mu_j$ of the arm with the best sample mean
$\overline X_j=\max_i\overline X_i$:
\begin{equation}
\label{eqn:simple-regret}
\IE[R]=\sum_{j=1}^K\Delta_j\Pr(\overline X_j=\max_i\overline X_i)
\end{equation}
where $\Delta_j=\mu_*-\mu_j$.
Strategies that minimize the simple regret are called pure exploration
strategies \cite{Bubeck.pure}. Principles of rational metareasoning
\cite{Russell.right} suggest that at each step the arm with the great
value of information (VOI) must be pulled, and the sampling must be
stopped and a decision must be made when no arm has positive VOI. 

To estimate the VOI of pulling an arm, either a certain 
distribution of the rewards should be assumed (and updated based on
observed rewards), or a distribution-independent bound on the VOI can be
used as the VOI estimate. In this paper, we use {\em concentration inequalities}
to derive distribution-independent bounds on the VOI.

\section{Related Work}

\textit{TODO}

\section{Some Concentration Inequalities}

Let $X_1, \ldots, X_n$ be i.i.d. random variables with values from $[0,1]$,
$X=\frac 1 n \sum_{i=1}^n X_i$. Then 
\begin{description}
\item[Hoeffding's inequality \rm{\cite{Hoeffding.ineq}}:] 
\begin{equation}
\Pr(X-\IE[X] \ge a) \le \exp ( -2na^2)
\label{eqn:conc-hoeffding}
\end{equation}
\item[Empirical Bernstein's inequality
  \rm{\cite{MaurerPontil.benrstein}} \textrm{(derived in Appendix~\ref{app:deriv-conc-empbernstein})}:]
\begin{eqnarray}
\Pr(X-\IE[X] \ge a) &\le& 2\exp \left( - \frac {na^2} {\frac {14} {3}
                          \frac {n} {n-1}a+2\overline\sigma_n^2}\right)\nonumber\\
                    &\le& 2\exp \left( - \frac {na^2} {10a+2\overline\sigma_n^2}\right)
\label{eqn:conc-empbernstein}
\end{eqnarray}
where sample variance $\overline\sigma_n^2$ is
\begin{equation}
\overline\sigma_n^2=\frac 1 {n(n-1)} \sum_{1\le i < j\le n}(X_i-X_j)^2
\label{eqn:sample-variance}
\end{equation}
\end{description}
Bounds (\ref{eqn:conc-hoeffding}, \ref{eqn:conc-empbernstein}) are symmetrical
around the mean. Bound~(\ref{eqn:conc-empbernstein}) is tighter than
(\ref{eqn:conc-hoeffding}) for small $a$ and $\overline\sigma_n^2$. 

\section{Upper Bounds on Value of Information}

The intrinsic VOI $\Lambda_i$ of pulling an arm is the expected decrease
in the regret compared to selecting an arm without pulling any arm at
all. The \textit{myopic} VOI estimate is of limited applicability to
Monte Carlo sampling, since the effect of a single sample is small,
and the myopic VOI estimate will often be non-positive, resulting in premature
termination of the search. However, $\Lambda_i$ can be estimated as the intrinsic 
value of perfect information $\Lambda_i^p$ about the mean reward of the $i$th arm. Two
cases are possible:
\begin{itemize}
\item the arm ($\alpha$) with the highest sample mean is pulled, and the 
mean of the arm is lower than the sample mean the second-best arm ($\beta$);
\item another arm is pulled, and the mean of the arm is higher
than the current highest sample mean ($\alpha$).
\end{itemize}
$\Lambda_i^p$ can be bounded from above as the probability that a
different arm is selected, multiplied by the
maximum possible increase in the reward:

\begin{thm} The intrinsic value of perfect information $\Lambda_i^p$ about the $i$th arm is
  bounded from above as
\begin{equation}
  \Lambda_i^p \le \left\{
  \begin{array}{l l}
    \overline X_\beta \Pr(\IE[X_i] \le \overline X_\beta) & \mbox{{\rm if} $i=\alpha$} \\
    (1-\overline X_\alpha)\Pr(\IE[X_i] \ge \overline X_\alpha) & \mbox{\rm otherwise}
  \end{array} \right.
\label{eqn:thm-vopi}
\end{equation}
\label{thm:vopi}
\end{thm}

\begin{proof} For the case $i\ne \alpha$, the probability that the perfect
  information about the $i$th arm changes the final choice is
  $\Pr(\IE[X_i] \ge \overline X_\alpha)$. $\IE[X_i] \le 1$ by definition,
  therefore the maximum increase in the expected reward is
  $(1-\overline X_\alpha)$. Thus the intrinsic value of perfect
  information is at most $\Pr(\IE[X_i] \ge \overline
  X_\alpha)(1-\overline X_\alpha)$.
  Proof for the case $i=\alpha$ is similar.
\end{proof}

The search time is finite, and in a simple case the \textit{search
  budget} specified as the maximum number of samples.
An estimate based on the perfect intrinsic VOI does not take in
consideration the remaining number of samples. Given two arms
with the same intrinsic perfect VOI, the VOI
estimate of the arm pulled fewer times so far should be higher.

\begin{dfn} The \textbf{blinkered estimate} of intrinsic VOI information of the
  $i$th arm is the intrinsic VOI of pulling the $i$th arm for the
  remaining budget.
\end{dfn}

\begin{thm} Denote the current number of samples of the $i$th arm as
  $n_i$. The blinkered estimate $\Lambda_i^b$ of intrinsic value of
  information of pulling the $i$th arm for the remaining budget of $N$
  samples is bounded from above as
\begin{equation}
  \Lambda_i^b \le \left\{
  \begin{array}{l l}
    \overline X_\beta\frac N {N+n_i}\Pr(\overline X_i'\le\overline X_\beta)
    \le N\frac {\overline X_\beta} {n_i}\Pr(\overline X_i'\le\overline X_\beta) & \mbox{{\rm if} $i=\alpha$} \\
    (1-\overline  X_\alpha)\frac N {N+n_i}\Pr(\overline X_i'\ge\overline X_\alpha)
       \le N\frac {(1-\overline  X_\alpha)} {n_i}\Pr(\overline X_i'\ge\overline X_\alpha) & \mbox{\rm otherwise}
  \end{array} \right.
\label{eqn:thm-be}
\end{equation}
where $\overline X_i'$ is the sample mean of the $i$th arm after $n_i+N$ 
samples.
\label{thm:be}
\end{thm}

\begin{proof}
The proof is similar to the proof of Theorem~\ref{thm:vopi}, with
$\overline X'_i$ substituted instead of $\IE[X_i]$, and, if
$i\ne\alpha$, the upper bound on $X_i'$ is $X_\alpha+(1-X_\alpha)
\frac N {N+n_i}$; if $i=\alpha$, the lower bound on $X_i'$ is
$X_\beta - X_\beta \frac N {N+n_i}$.
\end{proof}

The probabilities in equations (\ref{eqn:thm-vopi}, \ref{eqn:thm-be})
can be bounded from above using concentration inequalities. In
particular, Lemma~\ref{lemma:hoeffding-prob-bounds} (proved in 
Appendix~\ref{app:hoeffding-prob-bounds-proof}) is based on the
Hoeffding inequality (\ref{eqn:conc-hoeffding}):
\begin{lmm} The probabilities in equations (\ref{eqn:thm-vopi},
\ref{eqn:thm-be}) are bounded from above as
\begin{eqnarray}
\Pr(\IE[X_i] \le \overline X_\beta|i=\alpha)&
   \le & \exp(-2 (\overline X_i - \overline X_\beta)^2 n_i)\nonumber\\
\Pr(\IE[X_i] \ge \overline X_\alpha|i\ne\alpha)&
   \le & \exp(-2 (\overline X_\alpha - \overline X_i)^2 n_i)
\label{eqn:probound-perf-hoeffding}
\end{eqnarray}
\vspace{-1.5\baselineskip}
\begin{eqnarray}
\Pr(\overline X_i' \le \overline X_\beta|i=\alpha) &
   \le & 2\exp\left(- 2\left(\frac {1+\frac {n_i} N} {1+\sqrt {\frac {n_i} N}}(\overline X_i - \overline X_\beta)\right)^2 n_i \right)
   \le 2\exp\left(- 1.37(\overline X_i - \overline X_\beta)^2 n_i \right) \nonumber\\
\Pr(\overline X_i' \ge \overline X_\alpha|i\ne\alpha) &
   \le & 2\exp\left(- 2\left(\frac {1+\frac {n_i} N} {1+\sqrt {\frac {n_i} N}} (\overline X_\alpha - \overline X_i)\right)^2 n_i \right)
    \le 2\exp\left(- 1.37(\overline X_\alpha - \overline X_i)^2 n_i \right)
\label{eqn:probound-blnk-hoeffding}
\end{eqnarray}
\label{lemma:hoeffding-prob-bounds}
\end{lmm}

An upper bound on the intrinsic value of perfect information is obtained by substituting
(\ref{eqn:probound-perf-hoeffding}) into (\ref{eqn:thm-vopi}):
\begin{equation}
  \Lambda_i^p \le \hat\Lambda_i^p=\left\{
  \begin{array}{l l}
    \overline X_\beta\exp\left(- 2(\overline X_i - \overline X_\beta)^2 n_i \right) & \mbox{{\rm if} $i=\alpha$} \\
    (1-\overline  X_\alpha)\exp\left(- 2(\overline X_\alpha - \overline X_i)^2 n_i \right)(1-\overline  X_\alpha)  &  \mbox{\rm otherwise}
  \end{array} \right.
\label{eqn:bound-perf-hoeffding}
\end{equation}
An upper bound on the blinkered VOI estimate with ordering independent of $N$ is obtained
by substituting (\ref{eqn:probound-blnk-hoeffding}) into (\ref{eqn:thm-be}).
\begin{equation}
  \Lambda_i^b \le \hat\Lambda_i^b=\left\{
  \begin{array}{l l}
    C \frac {\overline X_\beta} {n_i}\exp\left(- 1.37(\overline X_i - \overline X_\beta)^2 n_i\right)
      & \mbox{{\rm if} $i=\alpha$} \\
    C \frac {(1-\overline  X_\alpha)} {n_i} \exp\left(- 1.37(\overline X_\alpha - \overline X_i)^2 n_i\right)
      &  \mbox{\rm otherwise}
  \end{array} \right.
\label{eqn:bound-blnk-hoeffding}
\end{equation}
where $C=2N$. The bound is generally tighter when $N$ is small relative to $n_i$,
and the estimate $\hat\Lambda_i^b$ can be viewed as a relaxation of the myopic VOI
estimate to longer sequences of actions, such
that the VOI estimate is positive for some of the actions.

Better bounds can be obtained through tighter estimates on the
probabilities, for example, based on the empirical Bernstein inequality
(\ref{eqn:conc-empbernstein}) or through a more careful application of
the Hoeffding inequality (Appendix~\ref{app:better-hoeffding-bound}).

\section{VOI-based Sampling Control}

\subsection{Selection Criterion}

Following the principles of rational metareasoning, an arm with
the highest VOI should be pulled at each step. The upper bounds
established in Theorems~\ref{thm:vopi},~\ref{thm:be} can be used
as VOI estimates. Blinkered VOI estimates~(\ref{eqn:thm-be},
 \ref{eqn:bound-blnk-hoeffding}) can be viewed
as approximations of VOI estimates for myopic or
receding horison policies. As illustrated by the empirical evaluation
(Section~\ref{sec:empirical-evaluation}), estimates based on upper
bounds on the VOI result in rational sampling policies, and exhibit
performance comparable to the performance of some state-of-the-art
heuristic algorithms.

\subsection{Termination Condition}
\label{sec:control-termination-condition}

The simplest termination condition for a sampling policy is the
budget---a fixed number of samples performed before a decision is
made. When a sample has a cost commensurable with the value of
information of a measurement, an upper bound on the intrinsic VOI can
be used to stop the sampling if the intrinsic VOI of any action
is less than the cost of sampling $C$:
\begin{equation}
\mbox{stop if } \max_i \Lambda_i \le C
\end{equation}
Blinkered VOI estimates~(\ref{eqn:thm-be},
\ref{eqn:bound-blnk-hoeffding}) include the remaining budget $N$ as a
factor, but given the cost of a single sample $c$, the cost of the
remaining samples accounted for in estimating the intrinsic VOI is
$C=cN$. $N$ can be dropped on both sides of the inequality,
and a viable stopping condition is
\begin{equation}
\begin{array}{c r r l l}
&\frac 1 N \Lambda_\alpha^b
\le\frac {\overline  X_\beta} {n_\alpha}\Pr(\overline X_\alpha'\le\overline X_\beta)&\le& c
&\nonumber\\
\mbox{and}&&&&\nonumber\\
&\frac 1 N \max_i\Lambda_i^b
\le \max_i\frac {(1-\overline  X_\alpha)} {n_i}\Pr(\overline X_i'\ge\overline X_\alpha)&\le& c
 & \forall i: i\ne\alpha
\end{array}
\label{eqn:stopping-blnk}
\end{equation}
The empirical evaluation (Section~\ref{sec:empirical-evaluation})
confirms viability of this stopping condition and illustrates the
influence of the sample cost $c$ on the performance of
the sampling policy.

\subsection{Sample Redistribution in Trees}
\label{sec:control-redistribution}

Monte-Carlo tree search \cite{Chaslot.montecarlo} selects at each
search step an action that appears to be the best based on outcomes
of \textit{search rollouts}. Two different criteria are employed in
action selection during a rollout:
\begin{itemize}
\item at the first step, the simple regret of selecting an action
must be minimized;
\item starting with the second step of a rollout, the expected reward
of the rollout
 must be made as close as possible to the optimum reward, so that the
 value of the action at the first step of the rollout is evaluated
 correctly. Thus, starting with the second step, the cumulative regret
 must be minimized.
\end{itemize}

A natural approach would be to apply an algorithm that minimizes the
simple regret in Multi-armed bandits at the first step, and the
cumulative regret (solves the exploration-exploitation trade-off)
during the rest of the rollout.  However, this approach assumes that
the information obtained from rollouts in the current state is
discarded after an action is selected. In practice, most successful
Monte-Carlo tree search algorithms re-use rollouts originating from an
earlier search state and passing through the current search state;
thus, the value of information of a rollout is determined not just by
the influence on the choice of the action at the current state, but
also by the influence on the choice at future search states, provided
the rollout passes through the search states which will be visited.

One way to account for this re-use would be to incroprorate the
`future' value of information into a VOI estimate. However, this 
approach appears to be complicated. Alternatively, one can behave
myopically in search tree depth:
\begin{enumerate}
\item estimate VOI as though the information is discarded after each step;
\item stop early if the VOI is below a certain threshold
   (see Section~\ref{sec:control-termination-condition});
\item save the unused sample budget for search in future state, such that
   if the nominal budget is $N$, and the unused budget in the last state
   is $N_u$, the search budget in the next state will be $N+N_u$.
\end{enumerate}
In this approach, the cost of a sample in the current state is the
VOI of increasing the budget of a future state by one sample.  It is
unclear whether this cost can be accurately estimated, but supposedly
a fixed value for a given problem type and algorithm implementation
would work. Indeed, the empirical evaluation (Section~\ref{sec:emp-go})
confirms that stopping and sample redistribution based on a learned
fixed cost  substantially improve the performance of the VOI-based
sampling policy in game tree search.


\section{Empirical Evaluation}
\label{sec:empirical-evaluation}

The experiments compare the UCT algorithm \cite{Kocsis.uct} with modified
versions of UCT in which the samples at the first step are selected according
to their VOI estimates. Three blinkered VOI estimates were used for the comparison:
\begin{description}
\item[VCT:] based on Hoeffding inequality (\ref{eqn:conc-hoeffding});
\item[ECT:] based on Hoeffding inequality with midpoint (see Appendix~\ref{app:better-hoeffding-bound});
\item[BCT:] based on empirical Bernstein inequality (\ref{eqn:conc-empbernstein}).
\end{description}
For simplicity, the bounds on probability for unlimited number of samples were used in the estimate formulas. The estimates where computed as follows:
\begin{eqnarray}\
  \Lambda_i^{VCT}& =&\left\{
  \begin{array}{l l}
    \frac {\overline X_\beta} {n_i}\exp\left(- 2(\overline X_i - \overline X_\beta)^2 n_i\right)
      & \mbox{{\rm if} $i=\alpha$} \\
    \frac {(1-\overline  X_\alpha)} {n_i}\exp\left(- 2(\overline X_\alpha - \overline X_i)^2 n_i\right)
      &  \mbox{\rm otherwise}
  \end{array}  \right.\nonumber\\
  \Lambda_i^{ECT}& =&\min\left(\Lambda_i^{VCT}, \min_y\left\{
  \begin{array}{l l}
    \frac {\overline X_\beta-y} {n_i}\exp\left(-2 n_i(\overline X_i-\overline X_\beta)^2\right)
    +\frac {\overline X_\beta} {n_i}\exp\left(-2 n_i(\overline X_i-y)^2\right)
      & \mbox{{\rm if} $i=\alpha$} \\
\frac {y-\overline X_\alpha} {n_i}\exp\left(-2 n_i(\overline X_\alpha-\overline
  X_i)^2\right)+\frac {1-\overline X_\alpha} {n_i}\exp\left(-2 n_i(y-\overline X_i)^2\right)
      &  \mbox{\rm otherwise}
  \end{array} \right.\right) \nonumber\\
  \Lambda_i^{BCT}& =&\min\left(\Lambda_i^{VCT},\left\{
  \begin{array}{l l}
    2\frac {\overline X_\beta} {n_i}\exp\left(- \frac {(\overline X_i - \overline X_\beta)^2 n_i} {\frac {14} 3 \frac {n_i} {n_i-1}(\overline X_i - \overline X_\beta) + 2 \overline X_i(1-\overline X_i)} \right)
      & \mbox{{\rm if} $i=\alpha$} \\
    2\frac {(1-\overline  X_\alpha)} {n_i}\exp\left(- \frac {(\overline X_\alpha - \overline X_i)^2 n_i} {\frac {14} 3 \frac {n_i} {n_i-1}(\overline X_\alpha - \overline X_i) + 2 \overline X_i(1-\overline X_i)} \right)
      &  \mbox{\rm otherwise}
  \end{array} \right.\right)
\label{eqn:emp-estimates}
\end{eqnarray}

\subsection{Selecting The Best Arm}
\label{sec:emp-arm}

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{flat.pdf}
\caption{Random instances: regret vs. number of samples}
\label{fig:random-instances}
\end{figure}

The sampling policies are first compared on random multi-armed bandit 
problem instances, where the Monte Carlo Tree Search algorithm
is reduced to the sample selection policy at the first step: UCT is 
actually UCB1. In the problem of pure exploration in multi-armed bandits
\cite{Bubeck.pure}, the smaller is the exploration factor, the higher
is the expected simple regret; indeed, UCT exhibits the lowest simple
regret in the experiments for the default exploration factor $Cp=\frac
{\sqrt 2} 2$ \cite{Kocsis.uct}.

Figure~\ref{fig:random-instances} shows experiment results for
randomly-generated multi-armed bandits with 32 Bernoulli arms, with
the mean rewards of the arms distributed uniformly in the range $[0,
  1]$, for a range of sample budgets $32..2048$, with multiplicative
step of $2$. The experiment for each number of samples was repeated
20000 times. UCT is worse than any of the VOI-aware sampling policies,
for example, for 384 samples the simple regret of UCT is $\approx 40$
times greater than of VCT, and $\approx 100$ times greater than of
ECT. On average, it takes $2.5$ times as many samples for UCT to reach
the regret of VCT. Different numbers of arms and distributions of
means give similar results.

\subsection{Playing Go Against UCT}
\label{sec:emp-go}

One search domain in which Monte-Carlo tree search based on the UCT
sampling policy has been particularly successful is playing the Go
game \cite{Gelly.mogo}. This series of experiments compares
the winning rate of VOI-aware policies (VCT, ECT, BCT) against UCT 
in Go and investigates the effect of early stopping and sample
redistribution. 

For the experiments, a modified version of Pachi \cite{Braudis.pachi},
a state of the art Go program, was used:
\begin{itemize}
\item The UCT engine of Pachi was extended with VOI-aware sampling
  policies at the first step. 
\item The stopping condition for the VOI-aware policies was
  modified and based solely on the sample cost, specified as
  a constant parameter. 
\item The time-allocation mode based on the fixed number of samples
  was modified such that 
  \begin{enumerate}
    \item the same number of samples is available to
      the agent at each step, independently of the number of pre-simulated
      games;  
    \item if samples were unused at the current step,
      they become available at the next step.
  \end{enumerate}
\end{itemize}
While the UCT engine is not the most powerful engine of Pachi, it is still
a strong player. On the other hand, additional
features of more advanced engines would obstruct the Monte Carlo
sampling phenomena which are the subject of the experiment.

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{vct-wins.pdf}
\caption{Go: winning rate --- UCT against VCT}
\label{fig:uct-against-vct}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{ect-wins.pdf}
\caption{Go: winning rate --- UCT against ECT}
\label{fig:uct-against-ect}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{bct-wins.pdf}
\caption{Go: winning rate --- UCT against BCT}
\label{fig:uct-against-bct}
\end{figure}
The engines were compared on the 9x9 board, for 5000, 7000, 1000, and
15000 samples (game simulations) per ply, each experiment was repeated
1000 times. Figures~\ref{fig:uct-against-vct}--\ref{fig:uct-against-bct} show the
winning rate of UCT against VCT, ECT, BCT, correspondingly, vs. the
stopping threshold (if the maximum VOI of a sample is below the
threshold, the simulation is stopped, and a move is chosen). Each curve
in the figures corresponds to a certain number of samples per ply. The
lower the curve, the better is the corresponding policy compared to UCT.

As the results show, without sample redistribution UCT is as good as
better as the VOI-aware policies for lower numbers of samples per ply
(5000, 7000), and the advantage of VOI-aware policies for larger
number of samples is rather modest (e.g., ECT wins in 55\% of games for
10000 samples per ply). For the stopping threshold of $10^-6$,
however, the VOI-aware policies are almost always better than UCT
(only VCT for 5000 samples wins in less than 50\% of games), with
stronger policies (ECT, VCT) reaching the winning rate of 66\%.

In agreement with intuition
(Section~\ref{sec:control-redistribution}), VOI-based stopping and sample
redistribution is most influential for medium numbers of samples per
ply. When the maximum number of samples is too low, early stopping would
result in poorly selected moves. On the other hand, when the maximum
number of samples is sufficiently high, the VOI of increasing the
maximum number of samples in a future state is low.

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{bests-bw.pdf}
\caption{Go: best winning rate comparison}
\label{fig:best-winning-rate}
\end{figure}
Figure~\ref{fig:best-winning-rate} presents relative performance of
different VOI-aware policies depending on the number of samples per
ply by comparing the best achieved winning rates against UCT (again,
the lower the bar, the better is the policy). Both ECT (based on the
Hoeffding inequality with midpoint) and BCT (based on the empirical
Benrstein inequality) are better than VCT. The difference between ECT
and VCT is most prominent for smaller number of samples, and the
winning rate of ECT increases relatively slowly with the number of
samples. In contrast, BCT is worse than ECT and only slightly better
than VCT for smaller number of samples, but the winning rate rapidly
increases with the number of samples, and for 10000 and 15000 samples
per ply BCT is the best of the policies with the winning rate reaching
67\%.

\section{Summary and Future Work}

\textit{TODO}

\clearpage

\appendix

\section{Proof of Lemma~\ref{lemma:hoeffding-prob-bounds}}
\label{app:hoeffding-prob-bounds-proof}

\begin{proof}[]
Equations (\ref{eqn:probound-perf-hoeffding}) is a direct
application of the Hoeffding inequality (\ref{eqn:conc-hoeffding}).

Equations (\ref{eqn:probound-blnk-hoeffding}) follow from the
observation that if $i\ne\alpha$, $\overline X_i'>\overline X_\alpha$
if and only if the mean $\overline X_i^+$ of $N$ samples from $n_i+1$
to $N$ is at least $\overline X_\alpha+(\overline X_\alpha-\overline
X_i)\frac {n_i} N$.

For any $\delta$, the probability that $\overline X_i'$ is \textit{not
greater} than
$X_\alpha$ is more than the probability that $\IE[X_i]\le\overline
X_i+\delta$ \textit{and} $\overline X_i'\ge \IE[X_i]+\overline X_\alpha - \overline
  X_i - \delta +(\overline X_\alpha - \overline X_i)\frac {n_i} N$;
therefore, the probability that $\overline X_i'$ is \textit{greater}
than $X_\alpha$ is less than the probability that $\IE[X_i]\ge\overline
X_i+\delta$ \textit{or} $\overline X_i'\le \IE[X_i]+\overline X_\alpha - \overline
  X_i - \delta +(\overline X_\alpha - \overline X_i)\frac {n_i} N$.
Using the union bound,
\begin{equation}
\Pr(\overline X_i'\ge \overline X_\alpha)\le
  \Pr(\IE[X_i]-\overline X_i \ge \delta) +
  \Pr(\overline X_i^+ - \IE[X_i] \ge \overline X_\alpha - \overline
  X_i - \delta +(\overline X_\alpha - \overline X_i)\frac {n_i} N)
\end{equation}
Bounding the probabilities on the right side using the Hoeffding
inequality, obtain:
\begin{equation}
\Pr(\overline X_i'\ge \overline X_\alpha)\le
  \exp(-2\delta^2n_i)+\exp\left(-2\left((\overline X_\alpha - \overline
  X_i)\left(1+\frac {n_i} N\right) - \delta\right)^2N\right)
\end{equation}
$\delta^2n = -2\left((\overline X_\alpha - \overline X_i)
(1+\frac {n_i} N) - \delta\right)^2N$ when
$\delta=\frac {1+\frac {n_i} N} {1+\sqrt {\frac {n_i} N}} (\overline X_\alpha
- \overline X_i) \ge (\sqrt 2 - 1)(\overline X_\alpha-\overline X_i)$,
therefore
\begin{eqnarray}
\Pr(\overline X_i'\ge \overline X_\alpha) 
& \le & 2\exp\left(-2\left( \frac {1+\frac {n_i} N} {1+\sqrt {\frac {n_i} N}}(\overline X_\alpha - \overline X_i)\right)^2 n_i\right)\nonumber \\
& \le & 2\exp(-2(\sqrt 2 - 1)^2(\overline X_\alpha - \overline X_i)^2n_i)
 \le 2\exp(-1.37(\overline X_\alpha - \overline X_i)^2n_i)
\end{eqnarray}
Derivation for the case $i=\alpha$ is similar.
\end{proof}

\section{Empirical Bernstein Inequality}
\label{app:deriv-conc-empbernstein}

Theorem~4 in~\cite{MaurerPontil.benrstein} states that
\begin{equation}
\Pr\left(\IE[X]-\overline X_n
    \ge \sqrt { \frac {2\overline\sigma_n^2 \ln 2/\delta} n } + \frac {7 \ln 2/\delta} {3(n-1)}\right)\le \delta
\end{equation}
Therefore
\begin{equation}
\Pr\left(\IE[X]-\overline X_n \ge
              \sqrt { \left(\frac {7 \ln 2/\delta} {3(n-1)}\right)^2+\frac {2\overline\sigma_n^2 \ln 2/\delta} n }
                      + \frac {7 \ln 2/\delta} {3(n-1)}\right)
   \le \delta.
\end{equation}
$a=\sqrt { \left(\frac {7 \ln 2/\delta} {3(n-1)}\right)^2+\frac {2\overline\sigma_n^2 \ln 2/\delta} n } + \frac {7 \ln 2/\delta} {3(n-1)}$
 is a root of square equation $a^2-a\frac {14 \ln 2/\delta} {3(n-1)} -\frac {2\overline\sigma_n^2 \ln 2/\delta} n=0$
which, solved for $\delta\triangleq\Pr(\IE[X]-\overline X_n\ge a)$,
gives
\begin{equation*}
\Pr(\IE[X]-\overline X_n\ge a)\le 2\exp \left( - \frac {na^2} {\frac {14} {3} \frac {n} {n-1}a+2\overline\sigma_n^2}\right)
\end{equation*}
Other derivations, giving slightly different results, are possible.

\section{Better Hoeffding-Based Bound on Value of Perfect Information}
\label{app:better-hoeffding-bound}

Bounds (\ref{eqn:bound-perf-hoeffding}, \ref{eqn:bound-blnk-hoeffding}) have the form
\begin{equation}
  \Lambda_i \le \hat\Lambda_i=\left\{
  \begin{array}{l l}
    \exp\left(- \gamma(\overline X_i - \overline X_\beta)^2 n_i \right)(\overline X_\beta-B) & \mbox{{\rm if} $i=\alpha$} \\
    \exp\left(- \gamma(\overline X_\alpha - \overline X_i)^2 n_i \right)(A-\overline  X_\alpha)  &  \mbox{\rm otherwise}
  \end{array} \right.
\label{eqn:bound-generic-hoeffding}
\end{equation}
where $A$ is the upper bound on the posterior sample mean $\overline X_i'$ if $i\ne \alpha$, $B$ is the lower bound on $\overline X_i'$ if $i=\alpha$. For $i=\alpha$, the bound can be improved by selecting a midpoint
$B < y < \overline X_\beta$ and computing the bound as the sum of two parts:
\begin{itemize}
\item $\overline X_\beta-y$ multiplied by the probability that
  $\overline X_i' \le \overline X_\beta$;
\item $\overline X_\beta-B$ multiplied by the probability that $\overline X_i'\le
  y$.
\end{itemize}.
\begin{equation}
\Lambda_{i|i=\alpha} \le \hat \Lambda_{i|i=\alpha} =
 (\overline X_\beta-y)\exp\left(-\gamma n_i(\overline X_i-\overline X_\beta)^2\right)
 +(\overline X_\beta-B)\exp\left(-\gamma n_i(\overline X_i-y)^2\right)
\label{eqn:bound-twopart-hoeffding-alpha}
\end{equation}
If $y$ that minimizes $\hat\Lambda_{i|i=\alpha}$ exists, the minimum is achieved when
 $\frac {d\hat\Lambda_{i|i=\alpha}} {dy}=0$, that
is, when $y$ is the root of the following equation:
\begin{equation}
2\gamma n_i(\overline X_\beta-B)(\overline X_i-y)=\exp\left(-\gamma n_i\left(\frac {\overline X_i-\overline X_\beta}
    {\overline X_i-y}\right)^2\right)
\end{equation}
The derivation for the case $i\ne \alpha$ is obtained by substitution $1-A, 1-\overline X_i, 1-\overline X_\alpha, 1-y$ instead of
$B, \overline X_i, \overline X_\beta, y$ into (\ref{eqn:bound-twopart-hoeffding-alpha}):
\begin{equation}
\Lambda_{i|i\ne\alpha} \le \hat\Lambda_{i|i\ne\alpha} = (y-\overline X_\alpha)\exp\left(-\gamma n_i(\overline X_\alpha-\overline
  X_i)^2\right)+(A-\overline X_\alpha)\exp\left(-\gamma n_i(y-\overline X_i)^2\right)
\label{eqn:bound-twopart-hoeffding-rest}
\end{equation}
A closed-form solution for $y$ cannot be obtained, but given
$A, B, \overline X_\alpha, \overline X_\beta, n$, the value of $y$ can be efficiently
computed.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
