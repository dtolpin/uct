\documentclass{article}
\usepackage{algpseudocode}
\usepackage[ruled]{algorithm}
\usepackage{url}
\usepackage{framed}
\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}
\usepackage{geometry}

\geometry{margin=1.2in}

\newcommand {\mean} {\ensuremath {\mathop{\mathrm{mean}}}}
\newcommand {\median} {\ensuremath {\mathop{\mathrm{median}}}}
\newcommand {\N} {\ensuremath {\mathcal{N}}}
\newcommand {\IE} {\ensuremath {\mathbb{E}}}
\newcommand {\cov} {\ensuremath {\mathop{\mathrm{cov}}}}
\newcommand {\BEL} {\ensuremath {\mathop{\mathrm{BEL}}}}

\newtheorem{dfn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lmm}{Lemma}

\title{VOI-aware Monte Carlo Sampling in Trees}
\author {David Tolpin, Solomon Eyal Shimony \\
Department of Computer Science, \\
Ben-Gurion University of the Negev, Beer Sheva, Israel \\
\{tolpin,shimony\}@cs.bgu.ac.il}

\begin{document}

\maketitle

\begin{abstract}
Upper bounds for the VOI are provided for pure exploration in the
Multi-armed Bandit Problem. Sampling policies based on the upper
bounds are suggested. Empirical evaluation of the policies and
comparison to the UCB1 and UCT policies is provided
on random problem instances as well as on the Go game.
\end{abstract}


\section{Introduction and Definitions}

Taking a sequence of samples in order to minimize the
regret of a decision based on the samples is abstracted by the
{\em Multi-armed Bandit Problem.} In the Multi-armed Bandit problem
we have a set of $K$ arms. Each arm can be pulled multiple
times. When the $i$th arm is pulled, a random reward $X_i$ from an
unknown stationary distribution is returned.  The reward is bounded
between 0 and 1.

The simple regret of a sampling policy for the Multi-armed Bandit
Problem is the expected difference between the best expected reward
$\mu_*$ and the expected reward $\mu_j$ of the arm with the best sample mean
$\overline X_j=\max_i\overline X_i$:
\begin{equation}
\label{eqn:simple-regret}
\IE[R]=\sum_{j=1}^K\Delta_j\Pr(\overline X_j=\max_i\overline X_i)
\end{equation}
where $\Delta_j=\mu_*-\mu_j$.
Strategies that minimize the simple regret are called pure exploration
strategies \cite{Bubeck.pure}. Principles of rational metareasoning
\cite{Russell.right} suggest that at each step the arm with the great
value of information (VOI) must be pulled, and the sampling must be
stopped and a decision must be made when no arm has positive VOI. 

To estimate the VOI of pulling an arm, either a certain 
distribution of the rewards should be assumed (and updated based on
observed rewards), or a distribution-independent bound on the VOI can be
used as the VOI estimate. In this paper, we use {\em concentration inequalities}
to derive distribution-independent bounds on the VOI.

\section{Related Work}

\textit{TODO}

\section{Some Concentration Inequalities}

Let $X_1, \ldots, X_n$ be i.i.d. random variables with values from $[0,1]$,
$X=\frac 1 n \sum_{i=1}^n X_i$. Then 
\begin{description}
\item[Hoeffding's inequality \rm{\cite{Hoeffding.ineq}}:] 
\begin{equation}
\Pr(X-\IE[X] \ge a) \le \exp ( -2na^2)
\label{eqn:conc-hoeffding}
\end{equation}
\item[Empirical Bernstein's inequality
  \rm{\cite{MaurerPontil.benrstein}} \textrm{(derived in Appendix~\ref{app:deriv-conc-empbernstein})}:]
\begin{eqnarray}
\Pr(X-\IE[X] \ge a) &\le& 2\exp \left( - \frac {na^2} {\frac {14} {3}
                          \frac {n} {n-1}a+2\overline\sigma_n^2}\right)\nonumber\\
                    &\le& 2\exp \left( - \frac {na^2} {10a+2\overline\sigma_n^2}\right)
\label{eqn:conc-empbernstein}
\end{eqnarray}
where sample variance $\overline\sigma_n^2$ is
\begin{equation}
\overline\sigma_n^2=\frac 1 {n(n-1)} \sum_{1\le i < j\le n}(X_i-X_j)^2
\label{eqn:sample-variance}
\end{equation}
\end{description}
Bounds (\ref{eqn:conc-hoeffding}, \ref{eqn:conc-empbernstein}) are symmetrical
around the mean. Bound~(\ref{eqn:conc-empbernstein}) is tighter than
(\ref{eqn:conc-hoeffding}) for small $a$ and $\overline\sigma_n^2$. 

\section{Upper Bounds on Value of Information}

The intrinsic VOI $\Lambda_i$ of pulling an arm is the expected decrease
in the regret compared to selecting an arm without pulling any arm at
all. The \textit{myopic} VOI estimate is of limited applicability to
Monte Carlo sampling, since the effect of a single sample is small,
and the myopic VOI estimate will often be non-positive, resulting in premature
termination of the search. However, $\Lambda_i$ can be estimated as the intrinsic 
value of perfect information $\Lambda_i^p$ about the mean reward of the $i$th arm. Two
cases are possible:
\begin{itemize}
\item the arm ($\alpha$) with the highest sample mean is pulled, and the 
mean of the arm is lower than the sample mean the second-best arm ($\beta$);
\item another arm is pulled, and the mean of the arm is higher
than the current highest sample mean ($\alpha$).
\end{itemize}
$\Lambda_i^p$ can be bounded from above as the probability that a
different arm is selected, multiplied by the
maximum possible increase in the reward:

\begin{thm} The intrinsic value of perfect information $\Lambda_i^p$ about the $i$th arm is
  bounded from above as
\begin{equation}
  \Lambda_i^p \le \left\{
  \begin{array}{l l}
    \Pr(\IE[X_i] \le \overline X_\beta)\overline X_\beta & \mbox{if $i=\alpha$} \\
    \Pr(\IE[X_i] \ge \overline X_\alpha)(1-\overline X_\alpha) & \mbox{otherwise}
  \end{array} \right.
\label{eqn:thm-vopi}
\end{equation}
\label{thm:vopi}
\end{thm}

\begin{proof} For the case $i\ne \alpha$, the probability that the perfect
  information about the $i$th arm changes the final choice is
  $\Pr(\IE[X_i] \ge \overline X_\alpha)$. $\IE[X_i] \le 1$ by definition,
  therefore the maximum increase in the expected reward is
  $(1-\overline X_\alpha)$. Thus the intrinsic value of perfect
  information is at most $\Pr(\IE[X_i] \ge \overline
  X_\alpha)(1-\overline X_\alpha)$.
  Proof for the case $i=\alpha$ is similar.
\end{proof}

The search time is finite, and in a simple case the \textit{search
  budget} specified as the maximum number of samples.
An estimate based on the perfect intrinsic VOI does not take in
consideration the remaining number of samples. Given two arms
with the same intrinsic perfect VOI, the VOI
estimate of the arm pulled fewer times so far should be higher.

\begin{dfn} The \textbf{blinkered estimate} of intrinsic VOI information of the
  $i$th arm is the intrinsic VOI of pulling the $i$th arm for the
  remaining budget.
\end{dfn}

\begin{thm} Denote the current number of samples of the $i$th arm as
  $n_i$. The blinkered estimate $\Lambda_i^b$ of intrinsic value of
  information of pulling the $i$th arm for the remaining budget of $N$
  samples is bounded from above as
\begin{equation}
  \Lambda_i^b \le \left\{
  \begin{array}{l l}
    \Pr(\overline X_i'\le\overline X_\beta)\overline X_\beta\frac N {N+n_i}
       \le N\Pr(\overline X_i'\le\overline X_\beta)\frac {\overline X_\beta} {n_i} & \mbox{if $i=\alpha$} \\
    \Pr(\overline X_i'\ge\overline X_\alpha)(1-\overline  X_\alpha)\frac N {N+n_i}
       \le N\Pr(\overline X_i'\ge\overline X_\alpha)\frac {(1-\overline  X_\alpha)} {n_i} & \mbox{otherwise}
  \end{array} \right.
\label{eqn:thm-be}
\end{equation}
where $\overline X_i'$ is the sample mean of the $i$th arm after $n_i+N$ 
samples.
\label{thm:be}
\end{thm}

\begin{proof}
The proof is similar to the proof of Theorem~\ref{thm:vopi}, with
$\overline X'_i$ substituted instead of $\IE[X_i]$, and, if
$i\ne\alpha$, the upper bound on $X_i'$ is $X_\alpha+(1-X_\alpha)
\frac N {N+n_i}$; if $i=\alpha$, the lower bound on $X_i'$ is
$X_\beta - X_\beta \frac N {N+n_i}$.
\end{proof}

The probabilities in equations (\ref{eqn:thm-vopi}, \ref{eqn:thm-be})
can be bounded from above using concentration inequalities. In
particular, Lemma~\ref{lemma:hoeffding-prob-bounds} (proved in 
Appendix~\ref{app:hoeffding-prob-bounds-proof}) is based on the
Hoeffding inequality (\ref{eqn:conc-hoeffding}):
\begin{lmm} The probabilities in equations (\ref{eqn:thm-vopi},
\ref{eqn:thm-be}) are bounded from above as
\begin{eqnarray}
\Pr(\IE[X_i] \le \overline X_\beta|i=\alpha)&
   \le & \exp(-2 (\overline X_i - \overline X_\beta)^2 n_i)\nonumber\\
\Pr(\IE[X_i] \ge \overline X_\alpha|i\ne\alpha)&
   \le & \exp(-2 (\overline X_\alpha - \overline X_i)^2 n_i)
\label{eqn:probound-perf-hoeffding}
\end{eqnarray}
\vspace{-1.5\baselineskip}
\begin{eqnarray}
\Pr(\overline X_i' \le \overline X_\beta|i=\alpha) &
   \le & 2\exp\left(- 2\left(\frac {1+\frac {n_i} N} {1+\sqrt {\frac {n_i} N}}(\overline X_i - \overline X_\beta)\right)^2 n_i \right)
   \le 2\exp\left(- 1.37(\overline X_i - \overline X_\beta)^2 n_i \right) \nonumber\\
\Pr(\overline X_i' \ge \overline X_\alpha|i\ne\alpha) &
   \le & 2\exp\left(- 2\left(\frac {1+\frac {n_i} N} {1+\sqrt {\frac {n_i} N}} (\overline X_\alpha - \overline X_i)\right)^2 n_i \right)
    \le 2\exp\left(- 1.37(\overline X_\alpha - \overline X_i)^2 n_i \right)
\label{eqn:probound-blnk-hoeffding}
\end{eqnarray}
\label{lemma:hoeffding-prob-bounds}
\end{lmm}

An upper bound on the intrinsic value of perfect information is obtained by substituting
(\ref{eqn:probound-perf-hoeffding}) into (\ref{eqn:thm-vopi}):
\begin{equation}
  \Lambda_i^p \le \hat\Lambda_i^p=\left\{
  \begin{array}{l l}
    \exp\left(- 2(\overline X_i - \overline X_\beta)^2 n_i \right)\overline X_\beta & \mbox{if $i=\alpha$} \\
    \exp\left(- 2(\overline X_\alpha - \overline X_i)^2 n_i \right)(1-\overline  X_\alpha)  &  \mbox{otherwise}
  \end{array} \right.
\label{eqn:bound-perf-hoeffding}
\end{equation}
An upper bound on the blinkered VOI estimate with ordering independent of $N$ is obtained
by substituting (\ref{eqn:probound-blnk-hoeffding}) into (\ref{eqn:thm-be}).
\begin{equation}
  \Lambda_i^b \le \hat\Lambda_i^b=\left\{
  \begin{array}{l l}
    C \exp\left(- 1.37(\overline X_i - \overline X_\beta)^2 n_i\right)\frac {\overline X_\beta} {n_i}
      & \mbox{if $i=\alpha$} \\
    C \exp\left(- 1.37(\overline X_\alpha - \overline X_i)^2 n_i\right)\frac {(1-\overline  X_\alpha)} {n_i}
      &  \mbox{otherwise}
  \end{array} \right.
\label{eqn:bound-blnk-hoeffding}
\end{equation}
where $C=2N$. The bound is generally tighter when $N$ is small relative to $n_i$,
and the estimate $\hat\Lambda_i^b$ can be viewed as a relaxation of the myopic VOI
estimate to longer sequences of actions, such
that the VOI estimate is positive for some of the actions.

Better bounds can be obtained through tighter estimates on the
probabilities, for example, based on the empirical Bernstein inequality
(\ref{eqn:conc-empbernstein}) or through a more careful application of
the Hoeffding inequality (Appendix~\ref{app:better-hoeffding-bound}).

\section{VOI-based Sampling Control}

\subsection{Selection Criterion}

Following the principles of rational metareasoning, an arm with
the highest VOI should be pulled at each step. The upper bounds
established in Theorems~\ref{thm:vopi},~\ref{thm:be} can be used
as VOI estimates. Blinkered VOI estimates~(\ref{eqn:thm-be},
 \ref{eqn:bound-blnk-hoeffding}) can be viewed
as approximations of VOI estimates for myopic or
receding horison policies. As illustrated by the empirical evaluation
(Section~\ref{sec:empirical-evaluation}), estimates based on upper
bounds on the VOI result in rational sampling policies, and exhibit
performance comparable to the performance of some state-of-the-art
heuristic algorithms.

\subsection{Termination Condition}
\label{sec:control-termination-condition}

The simplest termination condition for a sampling policy is the
budget---a fixed number of samples performed before a decision is
made. When a sample has a cost commensurable with the value of
information of a measurement, an upper bound on the intrinsic VOI can
be used to stop the sampling if the intrinsic VOI of any action
is less than the cost of sampling $C$:
\begin{equation}
\mbox{stop if } \max_i \Lambda_i \le C
\end{equation}
Blinkered VOI estimates~(\ref{eqn:thm-be},
\ref{eqn:bound-blnk-hoeffding}) include the remaining budget $N$ as a
factor, but given the cost of a single sample $c$, the cost of the
remaining samples accounted for in estimating the intrinsic VOI is
$C=cN$. $N$ can be dropped on both sides of the inequality,
and a viable stopping condition is
\begin{equation}
\begin{array}{c r r l l}
&\frac 1 N \Lambda_\alpha^b
\le\Pr(\overline X_\alpha'\le\overline X_\beta)\frac {\overline  X_\beta} {n_\alpha}&\le& c
&\nonumber\\
\mbox{and}&&&&\nonumber\\
&\frac 1 N \max_i\Lambda_i^b
\le \max_i\Pr(\overline X_i'\ge\overline X_\alpha)\frac {(1-\overline  X_\alpha)} {n_i}&\le& c
 & \forall i: i\ne\alpha
\end{array}
\label{eqn:stopping-blnk}
\end{equation}
The empirical evaluation (Section~\ref{sec:empirical-evaluation})
confirms viability of this stopping condition and illustrates the
influence of the sample cost $c$ on the performance of
the sampling policy.

\subsection{Sample Redistribution in Trees}

Monte-Carlo tree search \cite{Chaslot.montecarlo} selects at each search step an action
that appears to be the best based on outcomes of \textit{search rollouts}. Two different
criteria are employed in action selection during a rollout:
\begin{itemize}
\item at the first step, the simple regret of selecting an action must be minimized;
\item starting with the second step of a rollout, the expected reward of the rollout
 must be made as close as possible to the optimum reward, so that the value of the
 action at the first step of the rollout is evaluated correctly. Thus, starting with
 the second step, the cumulative regret must be minimized.
\end{itemize}

A natural approach would be to apply an algorithm that minimizes the
simple regret in Multi-armed bandits at the first step, and the
cumulative regret (solves the exploration-exploitation trade-off)
during the rest of the rollout.  However, this approach assumes that
the information obtained from rollouts in the current state is
discarded after an action is selected. In practice, most successful
Monte-Carlo tree search algorithms re-use rollouts originating from an
earlier search state and passing through the current search state;
thus, the value of information of a rollout is determined not just by
the influence on the choice of the action at the current state, but
also by the influence on the choice at future search states, provided
the rollout passes through the search states which will be visited.

One way to account for this re-use would be to incroprorate the
`future' value of information into a VOI estimate. However, this 
approach appears to be complicated. Alternatively, one can behave
myopically in search tree depth:
\begin{itemize}
\item estimate VOI as though the information is discarded after each step;
\item stop early if the VOI is below a certain threshold
   (see Section~\ref{sec:control-termination-condition});
\item save the unused sample budget for search in future state, such that
   if the nominal budget is $N$, and the unused budget in the last state
   is $N_u$, the search budget in the next state will be $N+N_u$.
\end{itemize}
In this approach, the cost of a sample in the current state is the VOI
of increasing the budget of a future state by one sample. It is unclear
whether this cost can be accurately estimated, but supposedly a fixed value
for a given problem type and algorithm implementation would work. Indeed,
the empirical evaluation (Section~\ref{sec:emp-go}) confirms that a learned
fixed cost and sample redistribution substantially improve performance of
the VOI-based sampling policy in game tree search.

\section{Empirical Evaluation}
\label{sec:empirical-evaluation}

\subsection{Selecting The Best Arm}

Figure~\ref{fig:random-instances}

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{flat.pdf}
\caption{Random instances: regret vs. number of samples}
\label{fig:random-instances}
\end{figure}


\subsection{Playing Go Against UCT}
\label{sec:emp-go}

Figure~\ref{fig:uct-against-vct}, Figure~\ref{fig:uct-against-ect},
Figure~\ref{fig:uct-against-bct}, Figure~\ref{fig:best-winning-rate}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{vct-wins.pdf}
\caption{Go: winning rate --- UCT against VCT}
\label{fig:uct-against-vct}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{ect-wins.pdf}
\caption{Go: winning rate --- UCT against ECT}
\label{fig:uct-against-ect}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{bct-wins.pdf}
\caption{Go: winning rate --- UCT against BCT}
\label{fig:uct-against-bct}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{bests-bw.pdf}
\caption{Go: best winning rate comparison}
\label{fig:best-winning-rate}
\end{figure}

\section{Summary and Future Work}

\textit{TODO}

\clearpage

\appendix

\section{Proof of Lemma~\ref{lemma:hoeffding-prob-bounds}}
\label{app:hoeffding-prob-bounds-proof}

\begin{proof}[]
Equations (\ref{eqn:probound-perf-hoeffding}) is a direct
application of the Hoeffding inequality (\ref{eqn:conc-hoeffding}).

Equations (\ref{eqn:probound-blnk-hoeffding}) follow from the
observation that if $i\ne\alpha$, $\overline X_i'>\overline X_\alpha$
if and only if the mean $\overline X_i^+$ of $N$ samples from $n_i+1$
to $N$ is at least $\overline X_\alpha+(\overline X_\alpha-\overline
X_i)\frac {n_i} N$.

For any $\delta$, the probability that $\overline X_i'$ is \textit{not
greater} than
$X_\alpha$ is more than the probability that $\IE[X_i]\le\overline
X_i+\delta$ \textit{and} $\overline X_i'\ge \IE[X_i]+\overline X_\alpha - \overline
  X_i - \delta +(\overline X_\alpha - \overline X_i)\frac {n_i} N$;
therefore, the probability that $\overline X_i'$ is \textit{greater}
than than $X_\alpha$ is less than the probability that $\IE[X_i]\ge\overline
X_i+\delta$ \textit{or} $\overline X_i'\le \IE[X_i]+\overline X_\alpha - \overline
  X_i - \delta +(\overline X_\alpha - \overline X_i)\frac {n_i} N$.
Using the union bound,
\begin{equation}
\Pr(\overline X_i'\ge \overline X_\alpha)\le
  \Pr(\IE[X_i]-\overline X_i \ge \delta) +
  \Pr(\overline X_i^+ - \IE[X_i] \ge \overline X_\alpha - \overline
  X_i - \delta +(\overline X_\alpha - \overline X_i)\frac {n_i} N)
\end{equation}
Bounding the probabilities on the right side using the Hoeffding
inequality, obtain:
\begin{equation}
\Pr(\overline X_i'\ge \overline X_\alpha)\le
  \exp(-2\delta^2n_i)+\exp\left(-2\left((\overline X_\alpha - \overline
  X_i)\left(1+\frac {n_i} N\right) - \delta\right)^2N\right)
\end{equation}
$\delta^2n = -2\left((\overline X_\alpha - \overline X_i)
(1+\frac {n_i} N) - \delta\right)^2N$ when
$\delta=\frac {1+\frac {n_i} N} {1+\sqrt {\frac {n_i} N}} (\overline X_\alpha
- \overline X_i) \ge (\sqrt 2 - 1)(\overline X_\alpha-\overline X_i)$,
therefore
\begin{eqnarray}
\Pr(\overline X_i'\ge \overline X_\alpha) 
& \le & 2\exp\left(-2\left( \frac {1+\frac {n_i} N} {1+\sqrt {\frac {n_i} N}}(\overline X_\alpha - \overline X_i)\right)^2 n_i\right)\nonumber \\
& \le & 2\exp(-2(\sqrt 2 - 1)^2(\overline X_\alpha - \overline X_i)^2n_i)
 \le 2\exp(-1.37(\overline X_\alpha - \overline X_i)^2n_i)
\end{eqnarray}
Derivation for the case $i=\alpha$ is similar.
\end{proof}

\section{Empirical Bernstein Inequality}
\label{app:deriv-conc-empbernstein}

Theorem~4 in~\cite{MaurerPontil.benrstein} states that
\begin{equation}
\Pr\left(\IE[X]-\overline X_n
    \ge \sqrt { \frac {2\overline\sigma_n^2 \ln 2/\delta} n } + \frac {7 \ln 2/\delta} {3(n-1)}\right)\le \delta
\end{equation}
Therefore
\begin{equation}
\Pr\left(\IE[X]-\overline X_n \ge
              \sqrt { \left(\frac {7 \ln 2/\delta} {3(n-1)}\right)^2+\frac {2\overline\sigma_n^2 \ln 2/\delta} n }
                      + \frac {7 \ln 2/\delta} {3(n-1)}\right)
   \le \delta.
\end{equation}
$a=\sqrt { \left(\frac {7 \ln 2/\delta} {3(n-1)}\right)^2+\frac {2\overline\sigma_n^2 \ln 2/\delta} n } + \frac {7 \ln 2/\delta} {3(n-1)}$
 is a root of square equation $a^2-a\frac {14 \ln 2/\delta} {3(n-1)} -\frac {2\overline\sigma_n^2 \ln 2/\delta} n=0$
which, solved for $\delta\triangleq\Pr(\IE[X]-\overline X_n\ge a)$,
gives
\begin{equation*}
\Pr(\IE[X]-\overline X_n\ge a)\le 2\exp \left( - \frac {na^2} {\frac {14} {3} \frac {n} {n-1}a+2\overline\sigma_n^2}\right)
\end{equation*}
Other derivations, giving slightly different results, are possible.

\section{Better Hoeffding-Based Bound on Value of Perfect Information}
\label{app:better-hoeffding-bound}

Bounds (\ref{eqn:bound-perf-hoeffding}, \ref{eqn:bound-blnk-hoeffding}) have the form
\begin{equation}
  \Lambda_i \le \hat\Lambda_i=\left\{
  \begin{array}{l l}
    \exp\left(- \gamma(\overline X_i - \overline X_\beta)^2 n_i \right)(\overline X_\beta-B) & \mbox{if $i=\alpha$} \\
    \exp\left(- \gamma(\overline X_\alpha - \overline X_i)^2 n_i \right)(A-\overline  X_\alpha)  &  \mbox{otherwise}
  \end{array} \right.
\label{eqn:bound-generic-hoeffding}
\end{equation}
where $A$ is the upper bound on the posterior sample mean $\overline X_i'$ if $i\ne \alpha$, $B$ is the lower bound on $\overline X_i'$ if $i=\alpha$. For $i=\alpha$, the bound can be improved by selecting a midpoint
$B < y < \overline X_\beta$ and computing the bound as the sum of two parts:
\begin{itemize}
\item $\overline X_\beta-y$ multiplied by the probability that
  $\overline X_i' \le \overline X_\beta$;
\item $\overline X_\beta-B$ multiplied by the probability that $\overline X_i'\le
  y$.
\end{itemize}.
\begin{equation}
\Lambda_{i|i=\alpha} \le \hat \Lambda_{i|i=\alpha} =
 (\overline X_\beta-y)\exp\left(-\gamma n_i(\overline X_i-\overline X_\beta)^2\right)
 +(\overline X_\beta-B)\exp\left(-\gamma n_i(\overline X_i-y)^2\right)
\label{eqn:bound-twopart-hoeffding-alpha}
\end{equation}
If $y$ that minimizes $\hat\Lambda_{i|i=\alpha}$ exists, the minimum is achieved when
 $\frac {d\hat\Lambda_{i|i=\alpha}} {dy}=0$, that
is, when $y$ is the root of the following equation:
\begin{equation}
2\gamma n_i(\overline X_\beta-B)(\overline X_i-y)=\exp\left(-\gamma n_i\left(\frac {\overline X_i-\overline X_\beta}
    {\overline X_i-y}\right)^2\right)
\end{equation}
The derivation for the case $i\ne \alpha$ is obtained by substitution $1-A, 1-\overline X_i, 1-\overline X_\alpha, 1-y$ instead of
$B, \overline X_i, \overline X_\beta, y$ into (\ref{eqn:bound-twopart-hoeffding-alpha}):
\begin{equation}
\Lambda_{i|i\ne\alpha} \le \hat\Lambda_{i|i\ne\alpha} = (y-\overline X_\alpha)\exp\left(-\gamma n_i(\overline X_\alpha-\overline
  X_i)^2\right)+(A-\overline X_\alpha)\exp\left(-\gamma n_i(y-\overline X_i)^2\right)
\label{eqn:bound-twopart-hoeffding-rest}
\end{equation}
A closed-form solution for $y$ cannot be obtained, but given
$A, B, \overline X_\alpha, \overline X_\beta, n$, the value of $y$ can be efficiently
computed.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
