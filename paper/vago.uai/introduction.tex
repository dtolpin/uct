The broad family of sequential decision problems includes
combinatorial search problems, game playing, robotic path planning,
model-predictive control problems, Markov decision processes (MDP), whether fully or
partially observable, and a huge range of applications. In almost all
realistic instances, exact solution is intractable and approximate
methods are sought. Perhaps the most popular approach is to simulate
a limited number of possible future action sequences, 
in order to find a move in the current state that is (hopefully)
near-optimal. One of the most successful
examples of this approach are sampling algorithms for
playing the game of Go \citep{Gelly.mogo} based on the upper-confidence bound (UCB)
method \citep{Kocsis+Szepesvari:2006}. This paper examines the issue of sampling for move selection from
first principles, by defining an appropriate meta-level MDP and formally
analyzing some of its properties. Although
the meta-level MDP is also intractable, approximation schemes based on
provable value of information bounds for this problem are presented. 
These lead to practical sampling schemes that
are promising: they out-perform the UCB-based schemes,
as indicated by empirical results on both ``flat'' selection and
game-playing in Go.

As selection is extremely general \citep{TolpinShimony:2012}, we focus on the adversarial
game-playing domain as an example for clarity.
A typical game-playing algorithm explores a tree or graph of action sequences
with some limit on depth, using pruning methods to avoid irrelevant
subtrees; based on what it finds in the explored portion of the state space, the algorithm
then selects a move. This exploration is repeated after observing the move
made by the opponent (whether or not information from previous explorations
is reused is discussed in Section \ref{sec:control-redistribution}).

Clearly, it is desirable to select the best possible move using the
least possible amount of exploration. For a given amount of
exploration, decision quality can be improved by directing exploration
towards those actions sequences whose outcomes are helpful in selecting
a good move. Thus, the {\em metalevel} decision problem is to choose
what future action sequences to explore (or, more generally, what
deliberative computations to do), while the {\em object-level}
decision problem is to choose an action to execute in the real world.

That the metalevel decision problem can itself be formulated and
solved decision-theoretically was noted by \citet{Matheson:1968},
borrowing directly from the related concept of {\em information value
  theory}~\citep{Howard:1966}. In essence, computations can be
selected according to the expected improvement in decision quality
resulting from their execution. I. J.~\citet{Good:1968} independently
proposed using this idea to control search in chess, and later defined
``Type II rationality'' to refer to agents that optimally solve the
metalevel decision problem before acting. As interest in probabilistic
and decision-theoretic approaches in AI grew during the 1980s, several
authors explored these ideas further~\citep{Dean+Boddy:1988,Doyle:1988,Fehling+Breese:1988,Horvitz:1987b}.
Work by
\citet{Russell+Wefald:1988a,Russell+Wefald:1991a,Russell+Wefald:1991b}
formulated the metalevel sequential decision problem, employing an
explicit model of the results of computational actions, and applied
this to the control of game-playing search in Othello with encouraging
results.

An independent thread of research on metalevel control began with work
by \citet{Kocsis+Szepesvari:2006} on the UCT algorithm, which operates
in the context of {\em Monte Carlo tree search} (MCTS) algorithms.
In MCTS, each computation takes the form
of a simulating a randomized sequence of actions leading from a leaf of the
current tree to a terminal state. UCT is primarily a method for
selecting a leaf from which to conduct the next simulation, and
forms the core of the successful \textsc{MoGo} algorithm for Go 
playing \citep{Gelly+Silver:2011}.  The UCT algorithm is
based on the the theory of bandit problems \citep{Berry+Fristedt:1985} and the near-optimal
UCB1 bandit algorithm \citep{Auer+et+al:2002}. UCT applies
UCB1 recursively to select actions to perform within simulations.

It is natural to consider whether the two independent threads are
consistent; for example, are bandit algorithms such as UCB1 approximate
solutions to some particular case of the metalevel decision problem
defined by Russell and Wefald? The answer, perhaps surprisingly, is no.
The essential difference is that, in bandit problems, every trial involves
executing a real object-level action with real costs, whereas in the metareasoning
problem the trials are {\em simulations} whose cost is usually independent of the
utility of the action being simulated. 
Hence, in UCT, bandit algorithms are applied to problems that are not bandit problems.

One consequence of the mismatch is that bandit policies are
inappropriately biased away from exploring actions whose current
utility estimates are low.  Another consequence is the absence of any
notion of ``stopping'' in bandit algorithms, which are designed for
infinite sequences of trials.  A metalevel policy needs to decide
when to stop deliberating and execute a real action.

Analyzing the metalevel problem within an appropriate theoretical
framework ought to lead to more effective algorithms than those
obtained within the bandit framework.  For Monte Carlo computations,
in which samples are gathered to estimate the utilities of actions,
the metalevel decision problem is an instance of the {\em selection
problem} studied in statistics~\citep{Bechhofer:1954,Swisher+et+al:2003}.  Despite
recent work by \citet{Frazier+Powell:2010} and \citet{TolpinShimony:2012}, the theory of selection
problems is less well understood than that of bandit problems.
Accordingly, we present in \secrefs{sec:optimal}{sec:context}
a number of results concerning optimal policies for the general case
and for specific sampling distributions. In particular, we give finite
bounds on the number of samples collected by optimal policies for
Bernoulli and Gaussian distributions. We also provide a simple
counterexample to the intuitive conjecture that an optimal policy
should not spend more on deciding than the decision is worth; in fact,
it is possible for an optimal policy to compute forever. We also show
by a simple counterexample that optimal {\em index
policies}~\citep{Gittins:1989} do not necessarily exist for selection
problems.

Motivated by this theoretical analysis, we propose in \secrefs{approx-bayesian-section}{approx-nonbayesian-section}
two families of heuristic approximations, one for the
Bayesian case and one for the distribution-free setting.
We show empirically that these rules give better performance than UCB1 on 
a wide range of standard (non-sequential) selection problems.
\secref{mcts-section} shows similar results for the case of guiding Monte Carlo tree search
in the game of Go.
