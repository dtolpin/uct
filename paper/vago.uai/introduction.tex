The broad family of sequential decision problems includes
combinatorial search problems, game playing, robotic path planning,
model-predictive control problems, Markov decision processes (MDP), whether fully or
partially observable, and a huge range of applications. In almost all
realistic instances, exact solution is intractable and approximate
methods are sought. Perhaps the most popular approach is to simulate
a limited number of possible future action sequences, 
in order to find a move in the current state that is (hopefully)
near-optimal. In this paper, we develop a theoretical framework to examine the problem of selecting
{\em which} future sequences to simulate.
We derive a number of new results concerning optimal policies for this selection problem
as well as new heuristic policies for controlling Monte Carlo simulations.
As described below, these policies outperform previously published methods for
``flat'' selection and game-playing in Go.

The basic ideas behind our approach are best explained in a familiar
context such as game playing.  A typical game-playing algorithm
chooses a move by first exploring a tree or graph of move sequences
and then selecting the most promising move based on this exploration.
Classical algorithms typically explore in a fixed order, imposing a
limit on exploration depth and using pruning methods to avoid
irrelevant subtrees; they may also reuse some previous computations
(see Section \ref{sec:control-redistribution}).  Exploring unpromising
or highly predictable paths to great depth is often wasteful; for a
given amount of exploration, decision quality can be improved by
directing exploration towards those actions sequences whose outcomes
are helpful in selecting a good move. Thus, the {\em metalevel}
decision problem is to choose what future action sequences to explore
(or, more generally, what deliberative computations to do), while the
{\em object-level} decision problem is to choose an action to execute
in the real world.

That the metalevel decision problem can itself be formulated and
solved decision-theoretically was noted by \citet{Matheson:1968},
borrowing from the related concept of {\em information value
  theory}~\citep{Howard:1966}. In essence, computations can be
selected according to the expected improvement in decision quality
resulting from their execution. I. J.~\citet{Good:1968} independently
proposed using this idea to control search in chess, and later defined
``Type II rationality'' to refer to agents that optimally solve the
metalevel decision problem before acting. As interest in probabilistic
and decision-theoretic approaches in AI grew during the 1980s, several
authors explored these ideas further~\citep{Dean+Boddy:1988,Doyle:1988,Fehling+Breese:1988,Horvitz:1987b}.
Work by
\citet{Russell+Wefald:1988a,Russell+Wefald:1991a,Russell+Wefald:1991b}
formulated the metalevel sequential decision problem, employing an
explicit model of the results of computational actions, and applied
this to the control of game-playing search in Othello with encouraging
results.

An independent thread of research on metalevel control began with work
by \citet{Kocsis+Szepesvari:2006} on the UCT algorithm, which operates
in the context of {\em Monte Carlo tree search} (MCTS) algorithms.
In MCTS, each computation takes the form
of a simulation of a randomized sequence of actions leading from a leaf of the
current tree to a terminal state. UCT is primarily a method for
selecting a leaf from which to conduct the next simulation, and
forms the core of the successful \textsc{MoGo} algorithm for Go 
playing \citep{Gelly+Silver:2011}.  The UCT algorithm is
based on the the theory of bandit problems \citep{Berry+Fristedt:1985} and the asymptotically near-optimal
UCB1 bandit algorithm \citep{Auer+et+al:2002}. UCT applies
UCB1 recursively to select actions to perform within simulations.

It is natural to consider whether the two independent threads are
consistent; for example, are bandit algorithms such as UCB1
approximate solutions to some particular case of the metalevel
decision problem defined by Russell and Wefald? The answer, perhaps
surprisingly, is no.  The essential difference is that, in bandit
problems, every trial involves executing a real object-level action
with real costs, whereas in the metareasoning problem the trials are
{\em simulations} whose cost is usually independent of the utility of
the action being simulated.  Hence, as \citet{Audibert+al:2010}
and \citet{Bubeck+al:2011} have also noted, UCT applies bandit
algorithms to problems that are not bandit problems.

One consequence of the mismatch is that bandit policies are
inappropriately biased away from exploring actions whose current
utility estimates are low.  Another consequence is the absence of any
notion of ``stopping'' in bandit algorithms, which are designed for
infinite sequences of trials.  A metalevel policy needs to decide
when to stop deliberating and execute a real action.

Analyzing the metalevel problem within an appropriate theoretical
framework ought to lead to more effective algorithms than those
obtained within the bandit framework.  For Monte Carlo computations,
in which samples are gathered to estimate the utilities of actions,
the metalevel decision problem is an instance of the {\em selection
problem} studied in statistics~\citep{Bechhofer:1954,Swisher+et+al:2003}.  Despite
some recent work \citep{Frazier+Powell:2010,TolpinShimony:2012}, the theory of selection
problems is less well understood than that of bandit problems.
Most work has focused on the probability of selection error rather than
optimal policies in the Bayesian setting~\citep{Bubeck+al:2011}.
Accordingly, we present in \secrefs{sec:optimal}{sec:context}
a number of results concerning optimal policies for the general case
as well as specific finite
bounds on the number of samples collected by optimal policies for
Bernoulli arms with beta priors.  We also provide a simple
counterexample to the intuitive conjecture that an optimal policy
should not spend more on deciding than the decision is worth; in fact,
it is possible for an optimal policy to compute forever. We also show
by counterexample that optimal {\em index
policies}~\citep{Gittins:1989} may not exist for selection
problems.

Motivated by this theoretical analysis, we propose in \secrefs{approx-bayesian-section}{approx-nonbayesian-section}
two families of heuristic approximations, one for the
Bayesian case and one for the distribution-free setting.
We show empirically that these rules give better performance than UCB1 on 
a wide range of standard (non-sequential) selection problems.
\secref{mcts-section} shows similar results for the case of guiding Monte Carlo tree search
in the game of Go.

%% (Proofs omitted due to space constraints can be found in the supplementary material.)