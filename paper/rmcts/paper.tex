\documentclass{article}
\usepackage{algpseudocode}
\usepackage[ruled]{algorithm}
\usepackage{url}
\usepackage{framed}
\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}
\usepackage{geometry}

\geometry{margin=1.2in}

\newcommand {\mean} {\ensuremath {\mathop{\mathrm{mean}}}}
\newcommand {\median} {\ensuremath {\mathop{\mathrm{median}}}}
\newcommand {\N} {\ensuremath {\mathcal{N}}}
\newcommand {\IE} {\ensuremath {\mathbb{E}}}
\newcommand {\cov} {\ensuremath {\mathop{\mathrm{cov}}}}
\newcommand {\BEL} {\ensuremath {\mathop{\mathrm{BEL}}}}

\newtheorem{lemma}{Lemma}

\title{Doing Better Than UCT: \\ Rational Monte Carlo Sampling in Trees}
\author {David Tolpin, Solomon Eyal Shimony \\
Department of Computer Science, \\
Ben-Gurion University of the Negev, Beer Sheva, Israel \\
\{tolpin,shimony\}@cs.bgu.ac.il}

\begin{document}

\maketitle

\begin{abstract}
UCT, a state-of-the art algorithm for Monte Carlo tree sampling
(MCTS), is based on UCB, a sampling policy for the Multi-armed Bandit
Problem (MAB) that minimizes the accumulated regret. However, MCTS
differs from MAB in that only the final choice, rather than all arm
pulls, brings a reward, that is, the simple regret, as opposite to the
accumulated regret, must be minimized. This work introduces policies for
multi-armed bandits with lower simple regret than UCB, and an
algorithm for MCTS which combines accumulated and simple regret
minization and outperforms UCT. Finite-time and asymptotic analysis of
the policies is provided, and the algorithms are empirically compared.
\end{abstract}


\section{Introduction}

The simple regret of a sampling policy for the Multi-armed Bandit
Problem is the expected difference between the best expected reward
$\mu_*$ and the expected reward of the arm with the best sample mean
$\mu_j,\;\overline X_j=\max_i\overline X_i$:
\begin{equation}
\label{eq:simple-regret}
\IE[R]=\sum_{j=1}^K\Delta_j\Pr(\overline X_j=\max_i\overline X_i)
\end{equation}
where $\Delta_j=\mu_*-\mu_j$.

\section{Main Results}

\subsection{Doing better than UCB}

\subsection{Doing better than UCT}


\section{Empirical Evaluation}

\subsection{Simple regret in multi-armed bandits}


\begin{figure}[ht]
  \begin{minipage}[c]{0.5\linewidth}
    \centering
    \includegraphics[scale=1.0]{onelevel-tree.pdf}\\
    \vspace{4em}
    a. search tree
  \end{minipage}
  \begin{minipage}[c]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.5]{flat-trilevel-k=64-uqb=8.pdf}\\
    b. regret vs. number of samples
  \end{minipage}
  \label{fig:mab-simple-regret}
  \caption{Simple regret in MAB}
\end{figure}

\subsection{Monte Carlo tree search}

\begin{figure}
  \begin{minipage}[c]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.8]{twolevel-tree.pdf}\\
    a. search tree
  \end{minipage}
  \begin{minipage}[c]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{tree-identity-k=16-uqb=8.pdf}\\ 
    b. 16 arms \\
    \vspace{1em}
    \includegraphics[scale=0.4]{tree-identity-k=64-uqb=8.pdf} \\
    c. 64 arms
 \end{minipage}
  \label{fig:mcts-regret}
  \caption{MCTS: a path to the best arm}
\end{figure}

\subsection{The sailing domain}


\begin{figure}[ht]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.45]{costs-size=6-group=median.pdf}\\
    a. median cost
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.45]{costs-size=6-group=minimum.pdf}\\
    b. minimum cost
  \end{minipage}
  \caption{The sailing domain, $6\times 6$ lake, cost vs. number of playouts}
  \label{fig:sailing-cost-vs-nsamples}
\end{figure}

\begin{figure}[ht]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.45]{costs-size=6-nsamples=199.pdf}\\
    a. 199 playouts\\
    \vspace{1em}
    \includegraphics[scale=0.45]{costs-size=6-nsamples=397.pdf}\\
    b. 397 playouts\\
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.45]{costs-size=6-nsamples=793.pdf}\\
    c. 793 playouts\\
    \vspace{1em}
    \includegraphics[scale=0.45]{costs-size=6-nsamples=1585.pdf}\\
    d. 1585 playouts\\
  \end{minipage}
  \caption{The sailing domain, $6\times 6$ lake, cost vs. factor}
  \label{fig:sailing-cost-vs-factor}
\end{figure}

\begin{figure}[ht]
  \begin{minipage}[b]{0.333\linewidth}
    \centering
    \includegraphics[scale=0.35]{costs-size=3-nsamples=397.pdf}\\
    a. $3\times 3$ lake
  \end{minipage}
  \begin{minipage}[b]{0.333\linewidth}
    \centering
    \includegraphics[scale=0.35]{costs-size=6-nsamples=397.pdf}\\
    b. $6\times 6$ lake
  \end{minipage}
  \begin{minipage}[b]{0.333\linewidth}
    \centering
    \includegraphics[scale=0.35]{costs-size=10-nsamples=397.pdf}\\
    b. $10\times 10$ lake
  \end{minipage}
  \caption{The sailing domain, 397 samples, cost vs. factor}
  \label{fig:sailing-lake-size}
\end{figure}

\begin{figure}[ht]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.45]{rcq-size=6-nsamples=199.pdf}\\
    a. 199 playouts\\
    \vspace{1em}
    \includegraphics[scale=0.45]{rcq-size=6-nsamples=397.pdf}\\
    b. 397 playouts\\
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.45]{rcq-size=6-nsamples=793.pdf}\\
    c. 793 playouts\\
    \vspace{1em}
    \includegraphics[scale=0.45]{rcq-size=6-nsamples=1585.pdf}\\
    d. 1585 playouts\\
  \end{minipage}
  \caption{The sailing domain, log vs. sqrt, $6\times 6$ lake}
  \label{fig:sailing-rcq-vs-factor}
\end{figure}
%
%\begin{figure}[ht]
%  \begin{minipage}[b]{0.5\linewidth} \centering
%    \includegraphics[scale=0.45,trim=0pt 0pt 0pt
%    0pt,clip]{sailing-size=13-factor=_5625.pdf}
%    {a. varying number of samples}
%  \end{minipage}
%  \begin{minipage}[b]{0.5\linewidth} \centering
%    \includegraphics[scale=0.45,trim=0pt 0pt 0pt
%    0pt,clip]{sailing-size=13-nsamples=240.pdf}
%    {b. varying exploration factor}
%  \end{minipage}
%  \caption{The sailing domain, a larger lake}
%  \label{fig:sailing-13}
%\end{figure}

\section{Summary and Future Work}

\section*{Acknowledgments}

The research is partially supported by Israel
Science Foundation grant 305/09, by the Lynne and William Frankel
Center for Computer Sciences, and by the Paul Ivanier Center for
Robotics Research and Production Management.

\bibliographystyle{plain}
\bibliography{refs}

\pagebreak

\appendix

\section{Facts}

{\bf Chernoff bound (see \cite{Hagerup.chernoff}):} for $m$ independent random variables $X_1, X_2, ..., X_m$
taking values 0 or 1, $X=\sum_{i=1}^m X_i$, and $0\le\delta\le 1$:

\begin{equation}
\label{eq:chernoff-bound}
\Pr[X < (1-\delta)\IE[X]] < e^{-\delta^2\IE[X]/2}
\end{equation}

\section{Derivations}

\subsection{Finite-time regret bounds}

For an analysis of pure exploration of uniform and UCB algorithms, see also
\cite{Bubeck.pure}.


\subsubsection{$\varepsilon$-greedy}

For $0<\varepsilon\le1-\frac 1 K$ and $x_0>0$ (see Section 3, Proofs, in \cite{Auer.ucb}):

\begin{eqnarray}
\Pr(\overline X_j=\max_i\overline X_i)&\le&2\left(x_0\cdot \Pr\left\{T_j^R(n)\le x_0\right\} + \frac 2{\Delta_j^2}e^{-\Delta_j^2\lfloor x_0 \rfloor/2}\right)\nonumber\\
&\le&2\left(x_0\cdot \Pr\left\{T_j^R(n)\le x_0\right\} + \frac {2 \cdot
  e^{\Delta_j^2/2}}{\Delta_j^2}e^{-\Delta_j^2 x_0 /2}\right)
\end{eqnarray}

Number of times $\mu$ the $j$th arm is selected {\it randomly}:

\begin{equation}
\mu\triangleq\IE\left[T_j^R(n)\right]=\frac {n\varepsilon} K
\end{equation}

By choosing $x_0=\frac \mu 2$ and applying the Chernoff bound (\ref{eq:chernoff-bound}), obtain:

\begin{eqnarray}
\label{eq:pr-epsilon-greedy}
\Pr(\overline X_j=\max_i\overline X_i)&\le& 2\left(\frac {\mu}{2} e^{-\mu/8} + \frac {2e^{\Delta_j^2/2}}{\Delta_j^2}e^{-\Delta_j^2 \mu/4}\right)\nonumber\\
&\le&\left(\mu + \frac {4\sqrt e}{\Delta_j^2}\right)e^{-\Delta_j^2\mu/8}
\end{eqnarray}

A bound on the simple regret $r_\varepsilon$ of an
$\varepsilon$-greedy policy follows from substituting
(\ref{eq:pr-epsilon-greedy}) into (\ref{eq:simple-regret}):

\begin{eqnarray}
  \IE r_\varepsilon&\le&\sum_{j=1}^K\Delta_j\left(\mu + \frac {4\sqrt e}
{\Delta_j^2}\right)e^{-\Delta_j^2\mu/8}\nonumber\\
&\le&K\left(\mu + \frac {4\sqrt
    e}{\Delta_{\min}}\right)e^{-\Delta_{\min}^2\mu/8}
\label{eq:deriv-epsilon-greedy-b}
\end{eqnarray}

\subsubsection{UCB($\alpha$)}

$n_*$ pulls of the best arm and $n_j$ pulls of the $j$th arm are
achieved simultaneously when either 


\begin{equation}
\sqrt {\frac {\alpha \log n} {n_j-1}} \ge 1 + \sqrt { \frac {\alpha \log n} {n_*}}
\end{equation}

or

\begin{equation}
\sqrt {\frac {\alpha \log n} {n_j}} \le 1 + \sqrt { \frac {\alpha \log n} {n_* - 1}}
\label{eq:deriv-ucb-alpha-a}
\end{equation}

Inequality (\ref{eq:deriv-ucb-alpha-a}) can be used to derive a lower
bound on the number of pulls of a non-optimal arm,  and, consequently, the upper
bound on the probability of selecting a non-optimal arm:

\begin{eqnarray}
\frac {\alpha \log n} {n_j}&\le& 1 + 2  \sqrt {\frac {\alpha \log n}
  {n_*-1}} + \frac {\alpha \log n} {n_*-1}\nonumber\\
n_j &\ge& \frac {\alpha \log n} {1+2 \sqrt {\frac {\alpha \log n}
    {n_*-1}} + \frac {\alpha \log n} {n_*-1}}\nonumber\\
    &\ge& \frac {\alpha \log n} {4} \mbox{ for } \alpha \log n \le n_*-1
\end{eqnarray}

The probability that the $j$th arm will be chosen can be bounded using
the Chernoff-Hoeffding bound:
\begin{equation}
P_j \le 2 \exp\left(-\frac {\alpha \Delta_j^2 \log n} 8 \right) = 2n^{\frac {-\alpha \Delta_j^2} 8}
\end{equation}

The expected simple regret of an UCB($\alpha$) algorithm is thus
bounded by the following inequality:
\begin{equation}
\IE r_{ucb} \le 2\sum_{j=1}^K \Delta_jn^{\frac {-\alpha \Delta_j^2} 8}
\label{eq:deriv-ucb-alpha-b}
\end{equation}

\cite{Bubeck.pure} establishes for UCB($\alpha$) a similar bound,
polynomial in the number of pulls $n$.

\subsection{Asymptotic regret analysis}

Bounds (\ref{eq:deriv-epsilon-greedy-b}, \ref{eq:deriv-ucb-alpha-b}) were
derived ignoring dependency of the allocation on expected values
of non-optimal arms. Derivation of better bounds seems
complicated. Instead, one can analyse symptotic behavior of each of the
algorithms in assumption that the sample mean is close to the expectation.

\subsubsection{$\varepsilon$-greedy}

A common technique \cite{Auer.ucb} to bound the probability of
selecting a non-optimal arm $j$ is to split the interval between
$\mu_j$ and $\mu_*$ at the middle and bound the probability of
selecting the $j$th arm by the sum of the probabilities that the samle
mean $\overline x_j$ of the $j$th arm is greater than $\mu_j+\frac
{\Delta_j} 2$ and the sample mean $\overline x_*$ of the optimal arm is
less than $\mu_*- \frac {\Delta_j} 2$. Each of the probabilities can
be bound using the Chernoff-Hoefding bound.

Splitting the interval between $\mu_j$ and $\mu_*$ half-way is an
arbirtrary decision. By selecting a different split point
depending on the allocation of samples, one can obtain a tighter
bound. On the other hand, there is a value of $\varepsilon$ that
minimizes the bound for the $\varepsilon$-greedy allocation scheme.
Thus, the  $\varepsilon$-greedy allocation scheme can be tuned by
finding $\varepsilon$ that minimizes the bound obtained by
selecting the best split point. 

Split the interval $[\mu_j, \mu_*]$ at $\mu_j+\delta_j$. The
probability $P_j$ for selecting the $j$th arm is bounded as:

\begin{equation}
P_j\le exp\left(-2\delta_j^2n_j\right)+exp\left(-2(\Delta_j-\delta_j)^2n_*\right)
\end{equation}

For the $\varepsilon$-greedy scheme, $n_i=\frac {\varepsilon n} K$,
$n_*=(1-\varepsilon)n$, thus

\begin{equation}
P_j \le \exp\left(-\frac {2\delta_j^2\varepsilon n} {K-1}\right) +\exp\left(-2(\Delta_j-\delta_j)^2(1-\varepsilon)n\right)
\end{equation}

In general, $\varepsilon$ and $\delta$ which minimize $P_j$ will
depend of $n$. For a constant $\varepsilon$, require that the
exponents in both terms are equal
\begin{eqnarray}
\exp\left(-\frac {2\delta_j^2\varepsilon n} {K-1}\right)
   &=& \exp\left(-2(\Delta_j-\delta_j)^2(1-\varepsilon)n\right)\\
\frac {\delta_j} {\Delta_j - \delta_j}
   &=& \sqrt { \frac {(K-1)(1-\varepsilon)} \varepsilon }
\label{eq:deriv-exps-equal}
\end{eqnarray}
and find conditional minima of $P_j(\delta_j, \epsilon)$ satisfying
either $\frac {\partial P_j} {\partial \delta_j}=0$ or $\frac {\partial P_j}
{\partial \varepsilon}=0$. 

\vspace{1em}
\paragraph{a. {\large $\frac {\partial P_j} {\partial \delta_j}=0$}}

\begin{eqnarray*}
\frac {4\delta_j\varepsilon n} {K-1}
  &-& 4(\Delta_j-\delta_j)(1-\varepsilon)n=0\\
\frac {\delta_j} {\Delta_j-\delta_j}&=&{ \frac {(K-1)(1-\varepsilon)} \varepsilon }
\end{eqnarray*}
and together with (\ref{eq:deriv-exps-equal}) this gives:
\begin{eqnarray}
\delta_j&=&\frac {\Delta_j} 2\nonumber\\
\varepsilon&=&1-\frac 1 K\nonumber\\
\IE r_{\varepsilon=1-1/K}&\le&B_{uniform}=2\exp\left(-\frac {\Delta_j^2n} {2K}\right)
\label{eq:deriv-uniform-sampling}
\end{eqnarray}

That is, the case $\frac {\partial P_j} {\partial \delta_j}=0$
corresponds to \emph{uniform random sampling}, and the tightest bound is obtained
by splitting the interval at the middle.


\paragraph{b. {\large $\frac {\partial P_j} {\partial \varepsilon}=0$}}

\begin{eqnarray*}
\frac {2\delta_j^2 n} {K-1}
  &-& 2(\Delta_j-\delta_j)^2n=0\\
\frac {\delta_j} {\Delta_j-\delta_j}&=&\sqrt {K-1}
\end{eqnarray*}
and together with (\ref{eq:deriv-exps-equal}) this gives:
\begin{eqnarray}
\frac {\delta_j} {\Delta_j}&=&\frac 1 {\sqrt {K-1}+1}\nonumber\\
\varepsilon&=&\frac 1 2\nonumber\\
\IE
r_{\varepsilon=1/2}&\le&B_{1/2\mbox{-}greedy}=2\exp\left(-\frac
  {\Delta_j^2n} {(\sqrt{K-1}+1)^2}\right)\nonumber\\
\lim_{K\to\infty}
\mathbb{B}r_{1/2\mbox{-}greedy}^K&=&2\exp\left(-{\Delta_j^2n}\right)\nonumber\\
\mbox{for large $K$: }\mathbb{B}r_{1/2\mbox{-}greedy}&\approx&2\exp\left(-\frac {\Delta_j^2n} K\right)
\label{eq:deriv-half-greedy-sampling}
\end{eqnarray}

That is, the case $\frac {\partial P_j} {\partial \varepsilon}=0$
corresponds to \emph{0.5-greedy sampling}, and for a large number of
arms $K$ 
\begin{equation}
\mathbb{B}r_{1/2\mbox{-}greedy}\approx \mathbb{B}r_{uniform}^2
\label{eq:derive-uniform-vs-half-greedy}
\end{equation}
Equation (\ref{eq:derive-uniform-vs-half-greedy}) suggests that
0.5-greedy sampling has a much higher convergence rate than uniform
sampling for a large number of arms.

\subsubsection{UCB($\alpha$)}

The $UCB(\alpha)$ allocation scheme distributes arm pulls such that
the upper bound on the probability of selecting a non-optimal arm is
asymptotically the same for all arms. Indeed, for large n
\begin{eqnarray}
n_i&\ll&n\\
n_*&\approx&n\\
\sqrt {\frac {2 \log n} {n_j}}&\approx&\Delta_j+\sqrt {\frac {2\log n}
  {n_*}} \\
&\Rightarrow& n_j \approx \frac {2\log n} {\Delta_j^2}
\label{eq:deriv-ucb-asymptotic-nj}
\end{eqnarray}
and by Hoeffding-Chernoff bound the upper bound $\mathbb{B}P_j$ on the probability
$P_j^{ucb}$ to choose the $j$th arm is
\begin{equation}
P_j^{ucb}\le\mathbb{B}P_j^{ucb}\approx2\exp\left(-2\Delta_j^2\frac {2\log n}{\Delta_j^2}\right)
             =2n^{-4}
\end{equation}

That is, UCB($\alpha$) supposedly distributes pulls of non-optimal
arms better than $\varepsilon$-greedy
scheme which may undersample arms with high expectation. The
polynomial, rather than exponential, convergence rate (Equation
\ref{eq:deriv-ucb-alpha-b}) is due to oversampling the best arm.

Replacing the logarithm with a faster growing subpolynomial
function, like square root (UQB), results in a superpolynomial convergence
rate of the upper bound:
\begin{equation}
P_j^{uqb}=\mathbb{B}P_j^{uqb}\approx2\exp\left(-\beta\Delta_j^2\frac {\sqrt n}{\Delta_j^2}\right)
            =2\exp(-2\beta\sqrt n)
 \end{equation}
where $\beta$ is a constant depending on reward bounds.
Finite time convergence can still be shown similarly to UCB.

\end{document}
