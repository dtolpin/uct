\documentclass{article}
\usepackage{algpseudocode}
\usepackage[ruled]{algorithm}
\usepackage{url}
\usepackage{framed}
\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}

\newcommand {\mean} {\ensuremath {\mathop{\mathrm{mean}}}}
\newcommand {\median} {\ensuremath {\mathop{\mathrm{median}}}}
\newcommand {\N} {\ensuremath {\mathcal{N}}}
\newcommand {\IE} {\ensuremath {\mathbb{E}}}
\newcommand {\cov} {\ensuremath {\mathop{\mathrm{cov}}}}
\newcommand {\BEL} {\ensuremath {\mathop{\mathrm{BEL}}}}

\newtheorem{lemma}{Lemma}

\title{Doing Better Than UCT: Rational Monte Carlo Sampling in Trees}
\author {David Tolpin \and Solomon Eyal Shimony \\
Department of Computer Science, \\
Ben-Gurion University of the Negev, Beer Sheva, Israel \\
\{tolpin,shimony\}@cs.bgu.ac.il}

\begin{document}

\maketitle

\begin{abstract}

Policies for the Multiarmed Bandit (MB) proved successful for Monte Carlo
tree sampling (MCTS). However, MCTS differs from MB in that only the
final choice, rather than all arm pulls, brings a reward. 

\end{abstract}


\section{Introduction}

\cite{Auer.ucb}

\section{Main Results}

\section{Doing Better Than UCB}

\section{Doing Better Than UCT}

\section{Summary and Future Work}

\section*{Acknowledgments}

The research is partially supported by Israel
Science Foundation grant 305/09, by the Lynne and William Frankel
Center for Computer Sciences, and by the Paul Ivanier Center for
Robotics Research and Production Management.

$\hat{C}_f$

\bibliographystyle{plain}
\bibliography{refs}

\pagebreak

\appendix

\section{Facts}

{\bf Chernoff bound:} for $m$ independent random variables $X_1, X_2, ..., X_m$
taking values 0 or 1, $X=\sum_{i=1}^m X_i$, and $0\le\delta\le 1$:

\begin{equation}
\label{eq:chernoff-bound}
\Pr[X < (1-\delta)\IE[X]] < e^{-\delta^2\IE[X]/2}
\end{equation}

\section{Finite-time regret bounds}

\subsection{$\varepsilon$-greedy}

Regret $R_\varepsilon$:

\begin{eqnarray}
\IE[R_\varepsilon]&=&\sum_{j=1}^K\Delta_j\Pr(\overline X_j=\max_i\overline X_i)
\end{eqnarray}

For $0<\varepsilon\le1-\frac 1 K$ and $x_0>0$ (see Section 3, Proofs, in \cite{Auer.ucb}):

\begin{eqnarray}
\Pr(\overline X_j=\max_i\overline X_i)&\le&2\left(x_0\cdot \Pr\left\{T_j^R(n)\le x_0\right\} + \frac 2{\Delta_j^2}e^{-\Delta_j^2\lfloor x_0 \rfloor/2}\right)\nonumber\\
&\le&2\left(x_0\cdot \Pr\left\{T_j^R(n)\le x_0\right\} + \frac {2 \cdot
  e^{\Delta_j^2/2}}{\Delta_j^2}e^{-\Delta_j^2 x_0 /2}\right)
\end{eqnarray}

Number of times $\mu$ the $j$th arm is selected {\it randomly}:

\begin{equation}
\mu\triangleq\IE\left[T_j^R(n)\right]=\frac {n\varepsilon} K
\end{equation}

By choosing $x_0=\frac \mu 2$ and applying the Chernoff bound (\ref{eq:chernoff-bound}), obtain:

\begin{eqnarray}
\Pr(\overline X_j=\max_i\overline X_i)&\le& 2\left(\frac {\mu}{2} e^{-\mu/8} + \frac {2e^{\Delta_j^2/2}}{\Delta_j^2}e^{-\Delta_j^2 \mu/4}\right)\nonumber\\
&\le&\left(\mu + \frac {4\sqrt e}{\Delta_j^2}\right)e^{-\Delta_j^2\mu/8}
\end{eqnarray}

\begin{eqnarray}
\IE[R_\varepsilon]&\le&\sum_{j=1}^K\Delta_j\left(\mu + \frac {4\sqrt e}
{\Delta_j^2}\right)e^{-\Delta_j^2\mu/8}\nonumber\\
&\le&K\left(\mu + \frac {4\sqrt e}{\Delta_{\min}}\right)e^{-\Delta_{\min}^2\mu/8}
\end{eqnarray}

\subsection{UCB}

Regret $R_{UCB}$:

\subsection{UVB}

Regret $R_{UVB}$:

\section{Asymptotic bounds for $\varepsilon$-greedy policies}


\section{Case study: $K-1$ and One Arms}

\end{document}
